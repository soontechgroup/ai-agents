# LangChain LLMåº”ç”¨æ¡†æ¶

## ğŸ“š ä½¿ç”¨è¯´æ˜

é¡¹ç›®ä½¿ç”¨ LangChain ä½œä¸º LLM åº”ç”¨å¼€å‘æ¡†æ¶ï¼Œæä¾›æ¨¡å—åŒ–çš„ç»„ä»¶æ¥æ„å»ºæ™ºèƒ½å¯¹è¯ç³»ç»Ÿã€‚

## ğŸ›  æ¡†æ¶é…ç½®

### å®‰è£…ä¾èµ–
```bash
pip install langchain langchain-openai
```

### åŸºæœ¬é…ç½®
```python
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain_core.messages import SystemMessage

# LLMåˆå§‹åŒ–
llm = ChatOpenAI(
    api_key="your-api-key",
    model="gpt-4o-mini",
    temperature=0.3
)

# Promptæ¨¡æ¿
prompt = ChatPromptTemplate.from_messages([
    SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹"),
    ("user", "{input}")
])

# è°ƒç”¨
response = llm.invoke(prompt.format_messages(input="ç”¨æˆ·æ¶ˆæ¯"))
```

## ğŸ’» é¡¹ç›®åº”ç”¨

### LLMæœåŠ¡åˆå§‹åŒ–
```python
# app/services/digital_human_training_service.py
class DigitalHumanTrainingService:
    def __init__(self):
        self.llm = ChatOpenAI(
            api_key=settings.OPENAI_API_KEY,
            model="gpt-4o-mini",
            temperature=0.3
        )
```

### æ„å›¾è¯†åˆ«
```python
def _recognize_intent(self, state: TrainingState) -> Dict[str, Any]:
    prompt = f"""
    åˆ†æç”¨æˆ·æ¶ˆæ¯çš„æ„å›¾å’Œå†…å®¹ç±»å‹ï¼š
    å½“å‰ç”¨æˆ·æ¶ˆæ¯: {state['current_message']}

    è¿”å›JSONæ ¼å¼ï¼š
    {{
        "intent": "information_sharing/question_asking/greeting/other",
        "stage": "initial/exploring/deepening/concluding"
    }}
    """

    response = self.llm.invoke([SystemMessage(content=prompt)])
    return json.loads(response.content)
```

### æ¶ˆæ¯ç³»ç»Ÿé›†æˆ
```python
# app/core/messages.py
from langchain_core.messages import BaseMessage

class UserMessage(BaseMessage):
    type: Literal["user"] = "user"

class AssistantMessage(BaseMessage):
    type: Literal["assistant"] = "assistant"
```

### æµå¼å¤„ç†
```python
# app/services/langgraph_service.py
class LangGraphService:
    def chat_stream(self, message: str, config: dict):
        prompt = self._build_chat_prompt(message, config)

        for chunk in self.llm.stream(prompt):
            if chunk.content:
                yield chunk.content
```

### çŸ¥è¯†æŠ½å–é›†æˆ
```python
# app/services/knowledge_extractor.py
class KnowledgeExtractor:
    def __init__(self):
        self.llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.1)

    async def extract_entities_and_relations(self, text: str):
        prompt = f"ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æå–å®ä½“å’Œå…³ç³»ï¼š\n{text}"
        response = self.llm.invoke([SystemMessage(content=prompt)])
        return json.loads(response.content)
```

LangChain ä¸ºé¡¹ç›®æä¾›äº†æ¨¡å—åŒ–çš„ LLM åº”ç”¨å¼€å‘åŸºç¡€ï¼Œæ”¯æŒæ™ºèƒ½å¯¹è¯å’ŒçŸ¥è¯†å¤„ç†ã€‚