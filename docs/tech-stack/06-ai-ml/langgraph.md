# LangGraph æœ‰çŠ¶æ€å·¥ä½œæµæ¡†æ¶

## ğŸ“š ä½¿ç”¨è¯´æ˜

é¡¹ç›®ä½¿ç”¨ LangGraph æ„å»ºæœ‰çŠ¶æ€çš„ AI å·¥ä½œæµï¼Œæ”¯æŒå¤æ‚çš„å¯¹è¯ç®¡ç†å’Œå¤šæ­¥éª¤å¤„ç†ã€‚

## ğŸ›  æ¡†æ¶é…ç½®

### å®‰è£…ä¾èµ–
```bash
pip install langgraph
```

### åŸºæœ¬é…ç½®
```python
from langgraph.graph import StateGraph, END
from langgraph.checkpoint.sqlite import SqliteSaver
from typing import TypedDict

# å®šä¹‰çŠ¶æ€ç»“æ„
class WorkflowState(TypedDict):
    messages: list
    current_step: str
    result: str

# åˆ›å»ºå·¥ä½œæµ
workflow = StateGraph(WorkflowState)

# æ·»åŠ èŠ‚ç‚¹
workflow.add_node("process", process_node)
workflow.add_node("analyze", analyze_node)

# è®¾ç½®æµç¨‹
workflow.set_entry_point("process")
workflow.add_edge("process", "analyze")
workflow.add_edge("analyze", END)

# ç¼–è¯‘å·¥ä½œæµ
app = workflow.compile()
```


## ğŸ’» é¡¹ç›®åº”ç”¨

### æ•°å­—äººè®­ç»ƒå·¥ä½œæµ
```python
# app/services/digital_human_training_service.py
class DigitalHumanTrainingFlow:
    def __init__(self, llm, memory_service):
        self.llm = llm
        self.memory_service = memory_service
        self.checkpointer = SqliteSaver.from_conn_string("sqlite:///training.db")

    class TrainingState(TypedDict):
        thread_id: str
        user_message: str
        intent: str
        memory_results: list
        response: str

    def build_workflow(self) -> StateGraph:
        workflow = StateGraph(self.TrainingState)

        # æ·»åŠ èŠ‚ç‚¹
        workflow.add_node("message_analysis", self.analyze_message)
        workflow.add_node("intent_recognition", self.recognize_intent)
        workflow.add_node("memory_search", self.search_memory)
        workflow.add_node("response_generation", self.generate_response)
        workflow.add_node("memory_storage", self.store_memory)

        # è®¾ç½®æµç¨‹
        workflow.set_entry_point("message_analysis")
        workflow.add_edge("message_analysis", "intent_recognition")
        workflow.add_edge("intent_recognition", "memory_search")
        workflow.add_edge("memory_search", "response_generation")
        workflow.add_edge("response_generation", "memory_storage")
        workflow.add_edge("memory_storage", END)

        return workflow.compile(checkpointer=self.checkpointer)
```

### æµå¼å“åº”é›†æˆ
```python
# app/api/v1/endpoints/conversations.py
class StreamingConversationAPI:
    async def stream_conversation(self, thread_id: str, message: str):
        async def generate_stream():
            initial_state = {
                "thread_id": thread_id,
                "user_message": message,
                "intent": "",
                "memory_results": [],
                "response": ""
            }

            config = {"configurable": {"thread_id": thread_id}}

            async for event in self.workflow.astream(initial_state, config):
                if "response_generation" in event:
                    yield f"data: {json.dumps(event['response_generation'])}\n\n"

        return StreamingResponse(generate_stream(), media_type="text/plain")
```

### çŠ¶æ€æŒä¹…åŒ–
```python
# app/core/checkpointer.py
class MySQLCheckpointer:
    def get(self, config: Dict[str, Any]) -> Optional[Checkpoint]:
        thread_id = config.get("configurable", {}).get("thread_id")

        # ä»æ•°æ®åº“åŠ è½½å†å²å¯¹è¯çŠ¶æ€
        latest_checkpoint = db.query(ConversationCheckpoint).filter(
            ConversationCheckpoint.thread_id == thread_id
        ).order_by(desc(ConversationCheckpoint.version)).first()

        # å¦‚æœæ²¡æœ‰checkpointï¼Œä»Messageè¡¨åŠ è½½å†å²æ¶ˆæ¯
        if not latest_checkpoint:
            messages = db.query(Message).filter(
                Message.user_id == user_id,
                Message.digital_human_id == digital_human_id
            ).order_by(Message.created_at).limit(50).all()
```

LangGraph ä¸ºé¡¹ç›®æä¾›äº†æœ‰çŠ¶æ€çš„å·¥ä½œæµç®¡ç†èƒ½åŠ›ï¼Œæ”¯æŒå¤æ‚çš„ AI å¯¹è¯å¤„ç†å’Œå¤šæ­¥éª¤ä»»åŠ¡ç¼–æ’ã€‚