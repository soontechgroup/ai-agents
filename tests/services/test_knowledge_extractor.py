import pytest
import os
import logging
import time
from app.services.knowledge_extractor import KnowledgeExtractor
from app.services.extraction_config import ExtractionConfig, ProcessingStrategy, ConfidenceMergeStrategy
from app.core.logger import logger

# è®¾ç½®ç¬¬ä¸‰æ–¹åº“æ—¥å¿—çº§åˆ«ä¸ºWARNINGï¼Œé¿å…DEBUGä¿¡æ¯
logging.getLogger("httpcore").setLevel(logging.WARNING)
logging.getLogger("httpx").setLevel(logging.WARNING)
logging.getLogger("openai").setLevel(logging.WARNING)
logging.getLogger("asyncio").setLevel(logging.WARNING)
logging.getLogger("urllib3").setLevel(logging.WARNING)
logging.getLogger("langchain").setLevel(logging.WARNING)
logging.getLogger("langchain_core").setLevel(logging.WARNING)
logging.getLogger("langchain_openai").setLevel(logging.WARNING)

# è®¾ç½®æ ¹æ—¥å¿—å™¨çº§åˆ«ä¸ºINFOï¼Œé¿å…DEBUGè¾“å‡º
logging.getLogger().setLevel(logging.INFO)

# çœŸå®APIæµ‹è¯• - éœ€è¦OPENAI_API_KEYç¯å¢ƒå˜é‡


@pytest.mark.integration
@pytest.mark.skipif(not os.getenv("OPENAI_API_KEY"), reason="éœ€è¦OPENAI_API_KEYç¯å¢ƒå˜é‡è¿›è¡ŒçœŸå®APIæµ‹è¯•")
class TestKnowledgeExtractorRealAPI:
    """çœŸå®APIæµ‹è¯• - ä½¿ç”¨GPT-4o-miniè¿›è¡Œå®é™…çŸ¥è¯†æŠ½å–"""
    
    def setup_method(self):
        """åˆå§‹åŒ–çœŸå®çš„KnowledgeExtractor"""
        logger.info("ğŸš€ å¼€å§‹çœŸå®APIæµ‹è¯• - ä½¿ç”¨GPT-4o-mini")
        self.extractor = KnowledgeExtractor()
        
        # éªŒè¯API keyå­˜åœ¨
        api_key = os.getenv("OPENAI_API_KEY", "")
        if api_key:
            masked_key = api_key[:8] + "..." + api_key[-4:] if len(api_key) > 12 else "***"
            logger.info(f"âœ… API Keyå·²è®¾ç½®: {masked_key}")
        
    @pytest.mark.asyncio
    async def test_prompt_format_compliance(self):
        """æµ‹è¯•LLMæ˜¯å¦éµå¾ªæˆ‘ä»¬å®šä¹‰çš„è¾“å‡ºæ ¼å¼"""
        logger.info("æ ¼å¼éµå¾ªåº¦æµ‹è¯•")
        
        test_text = "è‹¹æœå…¬å¸çš„iPhoneæ˜¯ä¸€æ¬¾æ™ºèƒ½æ‰‹æœºäº§å“ã€‚"
        logger.info(f"è¾“å…¥: {test_text}")
        
        # è·å–åŸå§‹LLMå“åº”
        chunks = self.extractor.text_splitter.split_text(test_text)
        if chunks:
            prompt = self.extractor._build_prompt(chunks[0])
            
            # ç›´æ¥è°ƒç”¨LLMè·å–åŸå§‹è¾“å‡º
            llm_response = await self.extractor.llm.ainvoke(prompt)
            raw_output = llm_response.content
            
            logger.info("LLMåŸå§‹è¾“å‡º:")
            logger.info(raw_output)
            
            # è§£æç»“æœ
            parsed_result = self.extractor._parse_output(raw_output)
            logger.info("è§£æåçš„ç»“æœ:")
            
            if parsed_result['entities']:
                logger.info("å®ä½“:")
                for entity in parsed_result['entities']:
                    name = entity.get('name', 'æœªçŸ¥')
                    # æ”¯æŒå¤šç±»å‹æ˜¾ç¤º
                    types = entity.get('types', [])
                    type_display = ", ".join(types) if types else entity.get('type', 'æœªçŸ¥ç±»å‹')
                    description = entity.get('description', '')
                    confidence = entity.get('confidence', 0.0)
                    
                    logger.info(f"  {name} ({type_display}) [{confidence:.2f}]")
                    
                    # æ˜¾ç¤ºç»“æ„åŒ–å±æ€§
                    properties = entity.get('properties', {})
                    if properties and properties != {'description': description}:
                        logger.info(f"    å±æ€§: {properties}")
            
            if parsed_result['relationships']:
                logger.info("å…³ç³»:")
                for rel in parsed_result['relationships']:
                    source = rel.get('source', 'æœªçŸ¥')
                    target = rel.get('target', 'æœªçŸ¥')
                    
                    # æ”¯æŒå¤šç±»å‹å…³ç³»æ˜¾ç¤º
                    types = rel.get('types', [])
                    rel_type = ", ".join(types) if types else rel.get('relation_type', 'æœªçŸ¥å…³ç³»')
                    
                    description = rel.get('description', '')
                    confidence = rel.get('confidence', 0.0)
                    strength = rel.get('strength', 0.0)
                    
                    logger.info(f"  {source} --[{rel_type}]--> {target} [{confidence:.2f}, {strength:.2f}]")
                    if description:
                        logger.info(f"    æè¿°: {description}")
                    
                    # æ˜¾ç¤ºå…³ç³»å±æ€§
                    properties = rel.get('properties', {})
                    if properties and properties != {'description': description}:
                        logger.info(f"    å±æ€§: {properties}")
            
            # ç®€å•éªŒè¯
            assert len(parsed_result['entities']) > 0 or len(parsed_result['relationships']) > 0
            
        logger.info("âœ… æ ¼å¼æµ‹è¯•å®Œæˆ")
        
    @pytest.mark.asyncio
    async def test_simple_entity_extraction(self):
        """æµ‹è¯•ç®€å•å®ä½“æŠ½å–"""
        logger.info("ç®€å•å®ä½“æŠ½å–æµ‹è¯•")
        
        test_text = "å¼ ä¸‰æ˜¯é˜¿é‡Œå·´å·´çš„å·¥ç¨‹å¸ˆï¼Œä»–åœ¨æ­å·å·¥ä½œã€‚"
        logger.info(f"è¾“å…¥: {test_text}")
        
        result = await self.extractor.extract(test_text)
        
        logger.info("æœ€ç»ˆæŠ½å–ç»“æœ:")
        if result['entities']:
            logger.info("å®ä½“:")
            for entity in result['entities']:
                name = entity.get('name', 'æœªçŸ¥')
                types = entity.get('types', [])
                type_display = ", ".join(types) if types else entity.get('type', 'æœªçŸ¥ç±»å‹')
                description = entity.get('description', '')
                confidence = entity.get('confidence', 0.0)
                
                logger.info(f"  {name} ({type_display}) [{confidence:.2f}]")
                
                properties = entity.get('properties', {})
                if properties and properties != {'description': description}:
                    logger.info(f"    å±æ€§: {properties}")
        
        if result['relationships']:
            logger.info("å…³ç³»:")
            for rel in result['relationships']:
                source = rel.get('source', 'æœªçŸ¥')
                target = rel.get('target', 'æœªçŸ¥')
                types = rel.get('types', [])
                rel_type = ", ".join(types) if types else rel.get('relation_type', 'æœªçŸ¥å…³ç³»')
                description = rel.get('description', '')
                confidence = rel.get('confidence', 0.0)
                strength = rel.get('strength', 0.0)
                
                logger.info(f"  {source} --[{rel_type}]--> {target} [{confidence:.2f}, {strength:.2f}]")
                if description:
                    logger.info(f"    æè¿°: {description}")
                
                properties = rel.get('properties', {})
                if properties and properties != {'description': description}:
                    logger.info(f"    å±æ€§: {properties}")
        
        # éªŒè¯åŸºæœ¬æŠ½å–æ•ˆæœ
        entity_names = [e.get('name', '').lower() for e in result['entities']]
        
        # æ£€æŸ¥å…³é”®å®ä½“ï¼ˆå…è®¸å˜ä½“ï¼‰
        has_person = any('å¼ ä¸‰' in name or 'zhangsan' in name for name in entity_names)
        has_company = any('é˜¿é‡Œå·´å·´' in name or 'alibaba' in name for name in entity_names)
        has_location = any('æ­å·' in name or 'hangzhou' in name for name in entity_names)
        
        assert len(result['entities']) >= 1, "åº”è¯¥è‡³å°‘è¯†åˆ«å‡º1ä¸ªå®ä½“"
        assert has_person or has_company or has_location, "åº”è¯¥è¯†åˆ«å‡ºå…³é”®å®ä½“"
        
        logger.info("âœ… ç®€å•æŠ½å–æµ‹è¯•å®Œæˆ")
        
    @pytest.mark.asyncio
    async def test_complex_business_scenario(self):
        """æµ‹è¯•å¤æ‚å•†ä¸šåœºæ™¯"""
        logger.info("å¤æ‚å•†ä¸šåœºæ™¯æµ‹è¯•")
        
        test_text = "é©¬æ–¯å…‹åœ¨2008å¹´æˆä¸ºç‰¹æ–¯æ‹‰çš„CEOï¼Œç‰¹æ–¯æ‹‰æ˜¯ä¸€å®¶ç”µåŠ¨æ±½è½¦åˆ¶é€ å…¬å¸ã€‚åŒæ—¶ï¼Œé©¬æ–¯å…‹è¿˜åˆ›ç«‹äº†SpaceXå…¬å¸ï¼Œä¸“æ³¨äºèˆªå¤©æŠ€æœ¯å¼€å‘ã€‚"
        logger.info(f"è¾“å…¥: {test_text}")
        
        result = await self.extractor.extract(test_text)
        
        logger.info("å¤æ‚åœºæ™¯æŠ½å–ç»“æœ:")
        if result['entities']:
            logger.info("å®ä½“:")
            for entity in result['entities']:
                name = entity.get('name', 'æœªçŸ¥')
                types = entity.get('types', [])
                type_display = ", ".join(types) if types else entity.get('type', 'æœªçŸ¥ç±»å‹')
                description = entity.get('description', '')
                confidence = entity.get('confidence', 0.0)
                
                logger.info(f"  {name} ({type_display}) [{confidence:.2f}]")
                
                properties = entity.get('properties', {})
                if properties and properties != {'description': description}:
                    logger.info(f"    å±æ€§: {properties}")
        
        if result['relationships']:
            logger.info("å…³ç³»:")
            for rel in result['relationships']:
                source = rel.get('source', 'æœªçŸ¥')
                target = rel.get('target', 'æœªçŸ¥')
                types = rel.get('types', [])
                rel_type = ", ".join(types) if types else rel.get('relation_type', 'æœªçŸ¥å…³ç³»')
                description = rel.get('description', '')
                confidence = rel.get('confidence', 0.0)
                strength = rel.get('strength', 0.0)
                
                logger.info(f"  {source} --[{rel_type}]--> {target} [{confidence:.2f}, {strength:.2f}]")
                if description:
                    logger.info(f"    æè¿°: {description}")
                
                properties = rel.get('properties', {})
                if properties and properties != {'description': description}:
                    logger.info(f"    å±æ€§: {properties}")
        
        # éªŒè¯å¤æ‚åœºæ™¯æŠ½å–
        entity_names = [e.get('name', '').lower() for e in result['entities']]
        
        # æ£€æŸ¥å…³é”®å®ä½“
        has_musk = any('é©¬æ–¯å…‹' in name or 'musk' in name for name in entity_names)
        has_tesla = any('ç‰¹æ–¯æ‹‰' in name or 'tesla' in name for name in entity_names)
        has_spacex = any('spacex' in name or 'å¤ªç©ºæ¢ç´¢' in name for name in entity_names)
        
        assert len(result['entities']) >= 2, "å¤æ‚åœºæ™¯åº”è¯¥è¯†åˆ«å‡ºå¤šä¸ªå®ä½“"
        assert has_musk or has_tesla or has_spacex, "åº”è¯¥è¯†åˆ«å‡ºå…³é”®å®ä½“"
        
        logger.info("âœ… å¤æ‚åœºæ™¯æµ‹è¯•å®Œæˆ")
        
    @pytest.mark.asyncio
    async def test_edge_cases(self):
        """æµ‹è¯•è¾¹ç•Œæƒ…å†µ"""
        logger.info("è¾¹ç•Œæƒ…å†µæµ‹è¯•")
        
        edge_cases = [
            ("ç©ºå­—ç¬¦ä¸²", ""),
            ("çº¯æ ‡ç‚¹", "ã€‚ï¼Œï¼ï¼Ÿï¼›ï¼š"),
            ("çŸ­æ–‡æœ¬", "ä½ å¥½ã€‚"),
        ]
        
        for case_name, test_text in edge_cases:
            logger.info(f"æµ‹è¯• {case_name}")
            logger.info(f"è¾“å…¥: '{test_text}'")
            
            try:
                result = await self.extractor.extract(test_text)
                
                logger.info("è¾¹ç•Œæƒ…å†µç»“æœ:")
                if result['entities']:
                    logger.info("å®ä½“:")
                    for entity in result['entities']:
                        name = entity.get('name', 'æœªçŸ¥')
                        types = entity.get('types', [])
                        type_display = ", ".join(types) if types else entity.get('type', 'æœªçŸ¥ç±»å‹')
                        confidence = entity.get('confidence', 0.0)
                        logger.info(f"  {name} ({type_display}) [{confidence:.2f}]")
                else:
                    logger.info("  æ— å®ä½“")
                
                if result['relationships']:
                    logger.info("å…³ç³»:")
                    for rel in result['relationships']:
                        source = rel.get('source', 'æœªçŸ¥')
                        target = rel.get('target', 'æœªçŸ¥')
                        types = rel.get('types', [])
                        rel_type = ", ".join(types) if types else rel.get('relation_type', 'æœªçŸ¥å…³ç³»')
                        confidence = rel.get('confidence', 0.0)
                        logger.info(f"  {source} --[{rel_type}]--> {target} [{confidence:.2f}]")
                else:
                    logger.info("  æ— å…³ç³»")
                
                # åŸºæœ¬æ–­è¨€
                assert isinstance(result, dict)
                assert 'entities' in result and 'relationships' in result
                
                logger.info(f"âœ… {case_name}æµ‹è¯•é€šè¿‡")
                
            except Exception as e:
                logger.warning(f"âš ï¸  {case_name}å‡ºç°å¼‚å¸¸: {str(e)}")
                
        logger.info("âœ… è¾¹ç•Œæƒ…å†µæµ‹è¯•å®Œæˆ")


@pytest.mark.integration
@pytest.mark.skipif(not os.getenv("OPENAI_API_KEY"), reason="éœ€è¦OPENAI_API_KEYç¯å¢ƒå˜é‡è¿›è¡ŒçœŸå®APIæµ‹è¯•")
class TestKnowledgeExtractorMultiChunk:
    """å¤šæ–‡æœ¬å—å¤„ç†æµ‹è¯• - GraphRAGå®Œæ•´åŠŸèƒ½æµ‹è¯•"""
    
    def setup_method(self):
        """åˆå§‹åŒ–å¤šç§é…ç½®çš„KnowledgeExtractor"""
        logger.info("ğŸ”§ åˆå§‹åŒ–å¤šæ–‡æœ¬å—å¤„ç†æµ‹è¯•")
        
        # é»˜è®¤å¢é‡å¤„ç†é…ç½®
        self.default_config = ExtractionConfig(
            strategy=ProcessingStrategy.INCREMENTAL,
            chunk_size=800,
            chunk_overlap=100,
            enable_context_enhancement=True,
            enable_cross_chunk_relations=True,
            min_entity_confidence=0.3,
            min_relationship_confidence=0.4
        )
        self.default_extractor = KnowledgeExtractor(self.default_config)
        
        # å¹¶è¡Œå¤„ç†é…ç½®
        self.parallel_config = ExtractionConfig(
            strategy=ProcessingStrategy.PARALLEL,
            chunk_size=600,
            chunk_overlap=50,
            max_concurrent_chunks=3,
            enable_cross_chunk_relations=True,
            min_entity_confidence=0.4
        )
        self.parallel_extractor = KnowledgeExtractor(self.parallel_config)
        
        # æ»‘åŠ¨çª—å£é…ç½®
        self.sliding_config = ExtractionConfig(
            strategy=ProcessingStrategy.SLIDING_WINDOW,
            chunk_size=500,
            chunk_overlap=150,
            enable_context_enhancement=True
        )
        self.sliding_extractor = KnowledgeExtractor(self.sliding_config)
        
    @pytest.mark.asyncio
    async def test_incremental_multi_chunk_processing(self):
        """æµ‹è¯•å¢é‡å¼å¤šæ–‡æœ¬å—å¤„ç†"""
        logger.info("ğŸ“Š å¢é‡å¼å¤šæ–‡æœ¬å—å¤„ç†æµ‹è¯•")
        
        # æ„é€ è·¨å¤šä¸ªå—çš„é•¿æ–‡æœ¬
        test_text = """
        é©¬æ–¯å…‹æ˜¯ç‰¹æ–¯æ‹‰å…¬å¸çš„é¦–å¸­æ‰§è¡Œå®˜ï¼Œä»–åœ¨2008å¹´åŠ å…¥ç‰¹æ–¯æ‹‰ï¼Œæˆä¸ºè¿™å®¶ç”µåŠ¨æ±½è½¦å…¬å¸çš„é¢†å¯¼è€…ã€‚ç‰¹æ–¯æ‹‰å…¬å¸æˆç«‹äº2003å¹´ï¼Œæ€»éƒ¨ä½äºç¾å›½åŠ åˆ©ç¦å°¼äºšå·å¸•æ´›é˜¿å°”æ‰˜ã€‚å…¬å¸ä¸“æ³¨äºç”µåŠ¨æ±½è½¦ã€èƒ½æºå­˜å‚¨å’Œå¤ªé˜³èƒ½æŠ€æœ¯çš„å¼€å‘ã€‚ç‰¹æ–¯æ‹‰çš„ä½¿å‘½æ˜¯åŠ é€Ÿä¸–ç•Œå‘å¯æŒç»­èƒ½æºçš„è½¬å˜ã€‚å…¬å¸æ¨å‡ºäº†å¤šæ¬¾ç”µåŠ¨æ±½è½¦äº§å“ï¼ŒåŒ…æ‹¬Model Sã€Model 3ã€Model Xå’ŒModel Yç­‰è½¦å‹ã€‚è¿™äº›è½¦å‹åœ¨å…¨çƒèŒƒå›´å†…å–å¾—äº†å·¨å¤§æˆåŠŸï¼Œæ”¹å˜äº†äººä»¬å¯¹ç”µåŠ¨æ±½è½¦çš„è®¤çŸ¥ã€‚
        
        é™¤äº†ç‰¹æ–¯æ‹‰ï¼Œé©¬æ–¯å…‹è¿˜åˆ›ç«‹äº†SpaceXå…¬å¸ã€‚SpaceXæ˜¯ä¸€å®¶ç§äººèˆªå¤©å…¬å¸ï¼Œæˆç«‹äº2002å¹´ï¼Œæ€»éƒ¨ä½äºåŠ åˆ©ç¦å°¼äºšå·éœæ¡‘ã€‚SpaceXå…¬å¸è‡´åŠ›äºå¼€å‘å¯é‡å¤ä½¿ç”¨çš„ç«ç®­æŠ€æœ¯ï¼Œå¹¶è®¡åˆ’è¿›è¡Œç«æ˜Ÿæ¢ç´¢ä»»åŠ¡ã€‚é©¬æ–¯å…‹æ‹…ä»»SpaceXçš„é¦–å¸­æ‰§è¡Œå®˜å…¼é¦–å¸­æŠ€æœ¯å®˜ã€‚SpaceXå·²ç»æˆåŠŸå¼€å‘äº†çŒé¹°9å·å’ŒçŒé¹°é‡å‹ç«ç®­ï¼Œå¹¶ä¸”å®ç°äº†ç«ç®­çš„å›æ”¶å’Œé‡å¤ä½¿ç”¨ã€‚å…¬å¸è¿˜å¼€å‘äº†è½½äººé¾™é£èˆ¹ï¼Œä¸ºå›½é™…ç©ºé—´ç«™æ‰§è¡Œè½½äººä»»åŠ¡ã€‚SpaceXçš„æœ€ç»ˆç›®æ ‡æ˜¯å®ç°ç«æ˜Ÿæ®–æ°‘ï¼Œä½¿äººç±»æˆä¸ºå¤šè¡Œæ˜Ÿç‰©ç§ã€‚
        
        æ­¤å¤–ï¼Œé©¬æ–¯å…‹è¿˜å‚ä¸åˆ›ç«‹äº†Neuralinkå…¬å¸ï¼Œè¿™æ˜¯ä¸€å®¶ä¸“æ³¨äºè„‘æœºæ¥å£æŠ€æœ¯çš„å…¬å¸ã€‚Neuralinkæˆç«‹äº2016å¹´ï¼Œç›®æ ‡æ˜¯å¼€å‘æ¤å…¥å¼è„‘æœºæ¥å£è®¾å¤‡ã€‚è¯¥å…¬å¸å¸Œæœ›é€šè¿‡å…ˆè¿›çš„ç¥ç»æŠ€æœ¯å¸®åŠ©æ²»ç–—å„ç§ç¥ç»ç–¾ç—…ï¼Œå¹¶æœ€ç»ˆå®ç°äººè„‘ä¸è®¡ç®—æœºçš„ç›´æ¥è¿æ¥ã€‚Neuralinkå·²ç»åœ¨åŠ¨ç‰©å®éªŒä¸­å–å¾—äº†é‡è¦è¿›å±•ï¼Œå±•ç¤ºäº†é€šè¿‡è„‘æœºæ¥å£æ§åˆ¶è®¡ç®—æœºè®¾å¤‡çš„å¯èƒ½æ€§ã€‚
        
        é©¬æ–¯å…‹è¿˜æ˜¯Boring Companyçš„åˆ›å§‹äººï¼Œè¿™å®¶å…¬å¸ä¸“æ³¨äºåœ°ä¸‹éš§é“äº¤é€šç³»ç»Ÿçš„å¼€å‘ã€‚Boring Companyæˆç«‹äº2016å¹´ï¼Œæ—¨åœ¨è§£å†³åŸå¸‚äº¤é€šæ‹¥å µé—®é¢˜ã€‚å…¬å¸å¼€å‘äº†é«˜é€Ÿåœ°ä¸‹éš§é“ç³»ç»Ÿï¼Œå¯ä»¥å¤§å¹…æé«˜åŸå¸‚äº¤é€šæ•ˆç‡ã€‚è¯¥å…¬å¸å·²ç»åœ¨æ‹‰æ–¯ç»´åŠ æ–¯ç­‰åŸå¸‚å»ºè®¾äº†ç¤ºèŒƒé¡¹ç›®ï¼Œè¯æ˜äº†åœ°ä¸‹äº¤é€šç³»ç»Ÿçš„å¯è¡Œæ€§ã€‚
        
        åœ¨é©¬æ–¯å…‹çš„é¢†å¯¼ä¸‹ï¼Œè¿™äº›å…¬å¸éƒ½åœ¨å„è‡ªé¢†åŸŸå–å¾—äº†çªç ´æ€§è¿›å±•ã€‚é©¬æ–¯å…‹ä»¥å…¶è¿œè§å“è¯†å’Œåˆ›æ–°ç²¾ç¥è€Œé—»åï¼Œè¢«è®¤ä¸ºæ˜¯å½“ä»£æœ€å…·å½±å“åŠ›çš„ä¼ä¸šå®¶ä¹‹ä¸€ã€‚ä»–çš„å·¥ä½œæ¶‰åŠå¤šä¸ªå‰æ²¿æŠ€æœ¯é¢†åŸŸï¼Œä»ç”µåŠ¨æ±½è½¦åˆ°å¤ªç©ºæ¢ç´¢ï¼Œä»äººå·¥æ™ºèƒ½åˆ°äº¤é€šè¿è¾“ï¼Œéƒ½åœ¨æ¨åŠ¨äººç±»ç¤¾ä¼šçš„è¿›æ­¥ã€‚
        """
        
        logger.info(f"è¾“å…¥æ–‡æœ¬é•¿åº¦: {len(test_text)} å­—ç¬¦")
        
        # ä½¿ç”¨extract_fullè¿›è¡Œå®Œæ•´å¤„ç†
        progress_updates = []
        def progress_callback(processed, total, entities, relations):
            progress_updates.append((processed, total, entities, relations))
            logger.info(f"è¿›åº¦: {processed}/{total} å—, å®ä½“: {entities}, å…³ç³»: {relations}")
        
        result = await self.default_extractor.extract_full(test_text, progress_callback)
        
        # éªŒè¯ç»“æœç»“æ„
        assert "entities" in result
        assert "relationships" in result
        assert "statistics" in result
        assert "config" in result
        assert "entity_merge_stats" in result
        assert "relationship_discovery_stats" in result
        
        # éªŒè¯å¤„ç†äº†å¤šä¸ªå—
        stats = result["statistics"]
        assert stats["total_chunks"] > 1, "åº”è¯¥åˆ†å‰²æˆå¤šä¸ªæ–‡æœ¬å—"
        assert stats["processed_chunks"] == stats["total_chunks"], "æ‰€æœ‰å—éƒ½åº”è¯¥è¢«å¤„ç†"
        
        # éªŒè¯å®ä½“æ•°é‡åˆç†
        entities = result["entities"]
        assert len(entities) >= 5, f"åº”è¯¥è¯†åˆ«å‡ºè¶³å¤Ÿçš„å®ä½“ï¼Œå½“å‰: {len(entities)}"
        
        # éªŒè¯å…³é”®å®ä½“
        entity_names = [e.get('name', '').lower() for e in entities]
        key_entities = ['é©¬æ–¯å…‹', 'musk', 'ç‰¹æ–¯æ‹‰', 'tesla', 'spacex']
        found_entities = [entity for entity in key_entities if any(entity in name for name in entity_names)]
        
        logger.info(f"è¯†åˆ«å‡ºçš„å®ä½“æ•°é‡: {len(entities)}")
        logger.info(f"æ‰¾åˆ°çš„å…³é”®å®ä½“: {found_entities}")
        
        for entity in entities[:3]:  # æ˜¾ç¤ºå‰3ä¸ªå®ä½“
            logger.info(f"  å®ä½“: {entity.get('name')} ({', '.join(entity.get('types', []))}) [{entity.get('confidence', 0):.2f}]")
        
        # éªŒè¯å…³ç³»
        relationships = result["relationships"]
        logger.info(f"è¯†åˆ«å‡ºçš„å…³ç³»æ•°é‡: {len(relationships)}")
        
        for rel in relationships[:3]:  # æ˜¾ç¤ºå‰3ä¸ªå…³ç³»
            logger.info(f"  å…³ç³»: {rel.get('source')} --[{', '.join(rel.get('types', []))}]--> {rel.get('target')} [{rel.get('confidence', 0):.2f}]")
        
        # éªŒè¯è¿›åº¦å›è°ƒè¢«è°ƒç”¨
        assert len(progress_updates) > 0, "è¿›åº¦å›è°ƒåº”è¯¥è¢«è°ƒç”¨"
        
        # éªŒè¯åˆå¹¶ç»Ÿè®¡
        merge_stats = result["entity_merge_stats"]
        assert "original_entities" in merge_stats
        assert "merged_entities" in merge_stats
        
        logger.info(f"å®ä½“åˆå¹¶ç»Ÿè®¡: {merge_stats}")
        logger.info(f"å…³ç³»å‘ç°ç»Ÿè®¡: {result['relationship_discovery_stats']}")
        
        logger.info("âœ… å¢é‡å¼å¤šæ–‡æœ¬å—å¤„ç†æµ‹è¯•å®Œæˆ")
        
    @pytest.mark.asyncio
    async def test_parallel_processing_strategy(self):
        """æµ‹è¯•å¹¶è¡Œå¤„ç†ç­–ç•¥"""
        logger.info("âš¡ å¹¶è¡Œå¤„ç†ç­–ç•¥æµ‹è¯•")
        
        test_text = """
        é˜¿é‡Œå·´å·´é›†å›¢æ˜¯ä¸­å›½æœ€å¤§çš„ç”µå­å•†åŠ¡å…¬å¸ä¹‹ä¸€ï¼Œç”±é©¬äº‘åœ¨1999å¹´åˆ›ç«‹äºæ­å·ã€‚
        å…¬å¸æ——ä¸‹æ‹¥æœ‰æ·˜å®ç½‘ã€å¤©çŒ«ã€æ”¯ä»˜å®ç­‰çŸ¥åå¹³å°ã€‚é©¬äº‘æ‹…ä»»äº†å¤šå¹´çš„è‘£äº‹å±€ä¸»å¸­ã€‚
        è…¾è®¯å…¬å¸æ˜¯å¦ä¸€å®¶ä¸­å›½ç§‘æŠ€å·¨å¤´ï¼Œç”±é©¬åŒ–è…¾åˆ›ç«‹äºæ·±åœ³ã€‚è…¾è®¯å¼€å‘äº†å¾®ä¿¡ã€QQç­‰ç¤¾äº¤è½¯ä»¶ã€‚
        ç™¾åº¦å…¬å¸ä¸“æ³¨äºæœç´¢å¼•æ“å’Œäººå·¥æ™ºèƒ½æŠ€æœ¯ï¼Œç”±æå½¦å®åˆ›ç«‹äºåŒ—äº¬ã€‚
        è¿™ä¸‰å®¶å…¬å¸è¢«ç§°ä¸ºä¸­å›½äº’è”ç½‘çš„BATä¸‰å·¨å¤´ã€‚
        """
        
        start_time = time.time()
        result = await self.parallel_extractor.extract_full(test_text)
        processing_time = time.time() - start_time
        
        logger.info(f"å¹¶è¡Œå¤„ç†è€—æ—¶: {processing_time:.2f}ç§’")
        
        # éªŒè¯é…ç½®æ­£ç¡®åº”ç”¨
        assert result["config"]["strategy"] == "parallel"
        
        # éªŒè¯ç»“æœè´¨é‡
        entities = result["entities"]
        relationships = result["relationships"]
        
        logger.info(f"å¹¶è¡Œå¤„ç†è¯†åˆ«å®ä½“: {len(entities)}")
        logger.info(f"å¹¶è¡Œå¤„ç†è¯†åˆ«å…³ç³»: {len(relationships)}")
        
        # éªŒè¯å…³é”®å®ä½“è¢«è¯†åˆ«
        entity_names = [e.get('name', '').lower() for e in entities]
        companies = ['é˜¿é‡Œå·´å·´', 'alibaba', 'è…¾è®¯', 'tencent', 'ç™¾åº¦', 'baidu']
        found_companies = [comp for comp in companies if any(comp in name for name in entity_names)]
        
        assert len(found_companies) >= 2, f"åº”è¯¥è¯†åˆ«å‡ºä¸»è¦å…¬å¸ï¼Œæ‰¾åˆ°: {found_companies}"
        
        logger.info("âœ… å¹¶è¡Œå¤„ç†ç­–ç•¥æµ‹è¯•å®Œæˆ")
        
    @pytest.mark.asyncio
    async def test_sliding_window_strategy(self):
        """æµ‹è¯•æ»‘åŠ¨çª—å£ç­–ç•¥"""
        logger.info("ğŸ”„ æ»‘åŠ¨çª—å£ç­–ç•¥æµ‹è¯•")
        
        test_text = """
        è‹¹æœå…¬å¸æˆç«‹äº1976å¹´ï¼Œç”±å²è’‚å¤«Â·ä¹”å¸ƒæ–¯ã€å²è’‚å¤«Â·æ²ƒå…¹å°¼äºšå…‹å’Œç½—çº³å¾·Â·éŸ¦æ©å…±åŒåˆ›ç«‹ã€‚
        å…¬å¸æ€»éƒ¨ä½äºåŠ åˆ©ç¦å°¼äºšå·åº“æ¯”è’‚è¯ºã€‚è‹¹æœæœ€è‘—åçš„äº§å“åŒ…æ‹¬iPhoneã€iPadã€Macç”µè„‘ç­‰ã€‚
        ä¹”å¸ƒæ–¯åœ¨1997å¹´é‡è¿”è‹¹æœåï¼Œæ¨å‡ºäº†ä¸€ç³»åˆ—é©å‘½æ€§äº§å“ã€‚iPhoneäº2007å¹´å‘å¸ƒï¼Œæ”¹å˜äº†æ™ºèƒ½æ‰‹æœºè¡Œä¸šã€‚
        è’‚å§†Â·åº“å…‹åœ¨2011å¹´æˆä¸ºè‹¹æœå…¬å¸CEOï¼Œç»§ç»­æ¨åŠ¨å…¬å¸å‘å±•ã€‚è‹¹æœç°åœ¨æ˜¯å…¨çƒå¸‚å€¼æœ€é«˜çš„å…¬å¸ä¹‹ä¸€ã€‚
        """
        
        result = await self.sliding_extractor.extract_full(test_text)
        
        # éªŒè¯é…ç½®æ­£ç¡®åº”ç”¨
        assert result["config"]["strategy"] == "sliding_window"
        
        # æ»‘åŠ¨çª—å£åº”è¯¥èƒ½æ›´å¥½åœ°å¤„ç†è·¨å—çš„è¿ç»­æ€§
        entities = result["entities"]
        relationships = result["relationships"]
        
        logger.info(f"æ»‘åŠ¨çª—å£è¯†åˆ«å®ä½“: {len(entities)}")
        logger.info(f"æ»‘åŠ¨çª—å£è¯†åˆ«å…³ç³»: {len(relationships)}")
        
        # éªŒè¯å…³é”®äººç‰©å’Œå…¬å¸è¢«è¯†åˆ«
        entity_names = [e.get('name', '').lower() for e in entities]
        key_entities = ['è‹¹æœ', 'apple', 'ä¹”å¸ƒæ–¯', 'jobs', 'åº“å…‹', 'cook']
        found_entities = [entity for entity in key_entities if any(key in name for name in entity_names)]
        
        assert len(found_entities) >= 2, f"åº”è¯¥è¯†åˆ«å‡ºå…³é”®å®ä½“ï¼Œæ‰¾åˆ°: {found_entities}"
        
        logger.info("âœ… æ»‘åŠ¨çª—å£ç­–ç•¥æµ‹è¯•å®Œæˆ")
        
    @pytest.mark.asyncio
    async def test_entity_merging_across_chunks(self):
        """æµ‹è¯•è·¨å—å®ä½“åˆå¹¶åŠŸèƒ½"""
        logger.info("ğŸ”€ è·¨å—å®ä½“åˆå¹¶æµ‹è¯•")
        
        # æ•…æ„åœ¨ä¸åŒå—ä¸­æåŠç›¸åŒå®ä½“çš„ä¸åŒè¡¨è¿°
        test_text = """
        åŸƒéš†Â·é©¬æ–¯å…‹æ˜¯ä¸€ä½è‘—åçš„ä¼ä¸šå®¶å’Œå·¥ç¨‹å¸ˆã€‚ä»–å‡ºç”Ÿäºå—éï¼Œåæ¥ç§»å±…ç¾å›½ã€‚
        é©¬æ–¯å…‹å…ˆç”Ÿåˆ›ç«‹äº†å¤šå®¶å…¬å¸ï¼ŒåŒ…æ‹¬ç‰¹æ–¯æ‹‰æ±½è½¦å…¬å¸ã€‚ç‰¹æ–¯æ‹‰æ˜¯ä¸€å®¶ä¸“æ³¨äºç”µåŠ¨æ±½è½¦çš„å…¬å¸ã€‚
        Elon Muskè¿˜åˆ›å»ºäº†SpaceXï¼Œè¿™æ˜¯ä¸€å®¶ç§äººèˆªå¤©å…¬å¸ã€‚é©¬æ–¯å…‹æ‹…ä»»SpaceXçš„CEOèŒä½ã€‚
        æ­¤å¤–ï¼ŒMr. Muskè¿˜å‚ä¸äº†å…¶ä»–é¡¹ç›®ï¼Œå¦‚è„‘æœºæ¥å£å…¬å¸Neuralinkã€‚
        """
        
        # ä½¿ç”¨è¾ƒå°çš„chunk_sizeç¡®ä¿åˆ†å‰²æˆå¤šä¸ªå—
        config = ExtractionConfig(
            strategy=ProcessingStrategy.INCREMENTAL,
            chunk_size=200,
            chunk_overlap=50,
            entity_similarity_threshold=0.7,  # é™ä½é˜ˆå€¼ä»¥æµ‹è¯•ç›¸ä¼¼å®ä½“åˆå¹¶
            enable_entity_aliasing=True
        )
        extractor = KnowledgeExtractor(config)
        
        result = await extractor.extract_full(test_text)
        
        entities = result["entities"]
        entity_names = [e.get('name', '') for e in entities]
        
        logger.info(f"åˆå¹¶åå®ä½“æ•°é‡: {len(entities)}")
        for entity in entities:
            logger.info(f"  {entity.get('name')} - ç½®ä¿¡åº¦: {entity.get('confidence', 0):.2f}")
        
        # éªŒè¯é©¬æ–¯å…‹çš„ä¸åŒè¡¨è¿°è¢«åˆå¹¶
        musk_entities = [name for name in entity_names if any(term in name.lower() for term in ['musk', 'é©¬æ–¯å…‹', 'elon'])]
        
        logger.info(f"è¯†åˆ«çš„é©¬æ–¯å…‹ç›¸å…³å®ä½“: {musk_entities}")
        
        # éªŒè¯åˆå¹¶ç»Ÿè®¡
        merge_stats = result["entity_merge_stats"]
        logger.info(f"åˆå¹¶å‰å®ä½“æ•°: {merge_stats.get('original_entities', 0)}")
        logger.info(f"åˆå¹¶åå®ä½“æ•°: {merge_stats.get('merged_entities', 0)}")
        logger.info(f"åˆå¹¶æ¯”ç‡: {merge_stats.get('merge_ratio', 0):.2f}")
        
        # åº”è¯¥å‘ç”Ÿäº†ä¸€äº›åˆå¹¶
        assert merge_stats.get("entities_saved", 0) > 0, "åº”è¯¥æœ‰å®ä½“è¢«åˆå¹¶"
        
        logger.info("âœ… è·¨å—å®ä½“åˆå¹¶æµ‹è¯•å®Œæˆ")
        
    @pytest.mark.asyncio
    async def test_cross_chunk_relationship_discovery(self):
        """æµ‹è¯•è·¨å—å…³ç³»å‘ç°"""
        logger.info("ğŸ”— è·¨å—å…³ç³»å‘ç°æµ‹è¯•")
        
        test_text = """
        å¼ ä¸‰æ˜¯ABCç§‘æŠ€å…¬å¸çš„è½¯ä»¶å·¥ç¨‹å¸ˆã€‚ä»–åœ¨åŒ—äº¬åŠå…¬å®¤å·¥ä½œã€‚
        ABCç§‘æŠ€å…¬å¸æ˜¯XYZé›†å›¢çš„å­å…¬å¸ã€‚XYZé›†å›¢æ€»éƒ¨ä½äºä¸Šæµ·ã€‚
        æå››ä¹Ÿåœ¨ABCç§‘æŠ€å…¬å¸å·¥ä½œï¼Œæ‹…ä»»äº§å“ç»ç†èŒä½ã€‚
        ç‹äº”æ˜¯XYZé›†å›¢çš„è‘£äº‹é•¿ï¼Œè´Ÿè´£æ•´ä¸ªé›†å›¢çš„æˆ˜ç•¥è§„åˆ’ã€‚
        """
        
        config = ExtractionConfig(
            strategy=ProcessingStrategy.INCREMENTAL,
            chunk_size=150,
            chunk_overlap=30,
            enable_cross_chunk_relations=True,
            relation_confidence_threshold=0.5
        )
        extractor = KnowledgeExtractor(config)
        
        result = await extractor.extract_full(test_text)
        
        relationships = result["relationships"]
        logger.info(f"å‘ç°å…³ç³»æ•°é‡: {len(relationships)}")
        
        # åˆ†æå…³ç³»ç±»å‹
        relation_methods = {}
        cross_chunk_relations = []
        
        for rel in relationships:
            discovery_method = rel.get('properties', {}).get('discovery_method', 'unknown')
            relation_methods[discovery_method] = relation_methods.get(discovery_method, 0) + 1
            
            if discovery_method in ['cooccurrence', 'transitive_inference']:
                cross_chunk_relations.append(rel)
                logger.info(f"  è·¨å—å…³ç³»: {rel.get('source')} --[{', '.join(rel.get('types', []))}]--> {rel.get('target')}")
                logger.info(f"    å‘ç°æ–¹æ³•: {discovery_method}")
        
        logger.info(f"å…³ç³»å‘ç°æ–¹æ³•ç»Ÿè®¡: {relation_methods}")
        
        # éªŒè¯å…³ç³»å‘ç°ç»Ÿè®¡
        discovery_stats = result["relationship_discovery_stats"]
        logger.info(f"å…³ç³»å‘ç°ç»Ÿè®¡: {discovery_stats}")
        
        # åº”è¯¥å‘ç°ä¸€äº›è·¨å—å…³ç³»
        if len(cross_chunk_relations) > 0:
            logger.info(f"æˆåŠŸå‘ç° {len(cross_chunk_relations)} ä¸ªè·¨å—å…³ç³»")
        
        logger.info("âœ… è·¨å—å…³ç³»å‘ç°æµ‹è¯•å®Œæˆ")
        
    @pytest.mark.asyncio
    async def test_context_enhancement(self):
        """æµ‹è¯•ä¸Šä¸‹æ–‡å¢å¼ºåŠŸèƒ½"""
        logger.info("ğŸ“ ä¸Šä¸‹æ–‡å¢å¼ºæµ‹è¯•")
        
        test_text = """
        å¾®è½¯å…¬å¸ç”±æ¯”å°”Â·ç›–èŒ¨åˆ›ç«‹äº1975å¹´ã€‚å…¬å¸æœ€åˆä¸“æ³¨äºè½¯ä»¶å¼€å‘ã€‚
        Windowsæ“ä½œç³»ç»Ÿæ˜¯å¾®è½¯æœ€è‘—åçš„äº§å“ä¹‹ä¸€ã€‚ç›–èŒ¨åœ¨å¾®è½¯æ‹…ä»»CEOå¤šå¹´ã€‚
        ç°åœ¨è¨è’‚äºšÂ·çº³å¾·æ‹‰æ˜¯å¾®è½¯çš„CEOã€‚çº³å¾·æ‹‰æ¨åŠ¨äº†å¾®è½¯å‘äº‘è®¡ç®—çš„è½¬å‹ã€‚
        Azureæ˜¯å¾®è½¯çš„äº‘è®¡ç®—å¹³å°ï¼Œä¸äºšé©¬é€ŠAWSç«äº‰æ¿€çƒˆã€‚
        """
        
        config = ExtractionConfig(
            strategy=ProcessingStrategy.INCREMENTAL,
            chunk_size=120,
            chunk_overlap=20,
            enable_context_enhancement=True,
            max_context_entities=8,
            context_window_size=2
        )
        extractor = KnowledgeExtractor(config)
        
        result = await extractor.extract_full(test_text)
        
        # éªŒè¯ä¸Šä¸‹æ–‡ç»Ÿè®¡
        stats = result["statistics"]
        logger.info(f"ä¸Šä¸‹æ–‡å¢å¼ºç»Ÿè®¡:")
        logger.info(f"  å…³é”®å®ä½“æ•°é‡: {stats.get('key_entities_count', 0)}")
        logger.info(f"  å¹³å‡æ¯å—å®ä½“æ•°: {stats.get('average_entities_per_chunk', 0):.1f}")
        logger.info(f"  æœ€å¸¸æåŠçš„å®ä½“: {stats.get('most_mentioned_entities', {})}")
        
        entities = result["entities"]
        logger.info(f"ä¸Šä¸‹æ–‡å¢å¼ºè¯†åˆ«å®ä½“: {len(entities)}")
        
        # éªŒè¯å¾®è½¯ç›¸å…³å®ä½“è¢«æ­£ç¡®è¯†åˆ«å’Œè¿æ¥
        entity_names = [e.get('name', '').lower() for e in entities]
        microsoft_related = ['å¾®è½¯', 'microsoft', 'ç›–èŒ¨', 'gates', 'çº³å¾·æ‹‰', 'nadella']
        found_ms_entities = [entity for entity in microsoft_related if any(ms in name for name in entity_names)]
        
        logger.info(f"è¯†åˆ«çš„å¾®è½¯ç›¸å…³å®ä½“: {found_ms_entities}")
        
        assert len(found_ms_entities) >= 2, "ä¸Šä¸‹æ–‡å¢å¼ºåº”è¯¥å¸®åŠ©è¯†åˆ«æ›´å¤šç›¸å…³å®ä½“"
        
        logger.info("âœ… ä¸Šä¸‹æ–‡å¢å¼ºæµ‹è¯•å®Œæˆ")
        
        
    @pytest.mark.asyncio
    async def test_processing_time_estimation(self):
        """æµ‹è¯•å¤„ç†æ—¶é—´ä¼°ç®—"""
        logger.info("â±ï¸ å¤„ç†æ—¶é—´ä¼°ç®—æµ‹è¯•")
        
        test_text = "è¿™æ˜¯ä¸€æ®µæµ‹è¯•æ–‡æœ¬ã€‚" * 100  # é‡å¤æ–‡æœ¬åˆ›å»ºè¾ƒé•¿å†…å®¹
        
        estimate = self.default_extractor.estimate_processing_time(test_text)
        
        logger.info(f"å¤„ç†æ—¶é—´ä¼°ç®—:")
        logger.info(f"  é¢„ä¼°æ—¶é—´: {estimate['estimated_time_seconds']:.1f}ç§’")
        logger.info(f"  é¢„ä¼°å—æ•°: {estimate['estimated_chunks']}")
        logger.info(f"  ä½¿ç”¨ç­–ç•¥: {estimate['strategy']}")
        logger.info(f"  æ–‡æœ¬é•¿åº¦: {estimate['text_length']}")
        
        # éªŒè¯ä¼°ç®—ç»“æœåˆç†
        assert estimate['estimated_time_seconds'] > 0
        assert estimate['estimated_chunks'] > 0
        assert estimate['text_length'] == len(test_text)
        
        logger.info("âœ… å¤„ç†æ—¶é—´ä¼°ç®—æµ‹è¯•å®Œæˆ")