from typing import Dict, List, Any, Generator, Optional, AsyncGenerator, TypedDict, Annotated, Tuple
import json
import uuid
from datetime import datetime
from langchain_openai import ChatOpenAI
from langchain_core.messages import BaseMessage
from app.core.messages import UserMessage, AssistantMessage, SystemMessage, serialize_message
from langgraph.graph import StateGraph, END
import operator

from app.services.knowledge_extractor import KnowledgeExtractor
from app.services.graph_service import GraphService
from app.services.hybrid_search_service import HybridSearchService
from app.repositories.training_message_repository import TrainingMessageRepository
from app.core.checkpointer import MySQLCheckpointer
from app.core.logger import logger
from app.core.config import settings
from app.core.models import DigitalHumanTrainingMessage


class TrainingState(TypedDict):
    messages: Annotated[List[BaseMessage], operator.add]
    digital_human_id: int
    user_id: int
    current_message: str
    extracted_knowledge: Dict[str, Any]
    knowledge_context: Dict[str, Any]
    memory_search_results: Dict[str, Any]  # è®°å¿†æœç´¢ç»“æœ
    next_question: str
    should_extract: bool
    should_explore_deeper: bool
    conversation_stage: str
    total_knowledge_points: int
    categories: Dict[str, Any]
    current_step: str
    completed_steps: Annotated[List[str], operator.add]
    step_results: Dict[str, Any]
    thinking_process: Annotated[List[str], operator.add]
    events: Annotated[List[Dict[str, Any]], operator.add]  # äº‹ä»¶é˜Ÿåˆ—ï¼Œç”¨äºæµå¼é€šçŸ¥


class DigitalHumanTrainingService:
    
    def __init__(
        self,
        training_message_repo: TrainingMessageRepository,
        # training_session_repo: TrainingSessionRepository,  # ç§»é™¤ä¼šè¯æ¦‚å¿µ
        knowledge_extractor: KnowledgeExtractor,
        graph_service: GraphService,
        hybrid_search_service: Optional[HybridSearchService] = None,
        db_session_factory=None
    ):
        self.training_message_repo = training_message_repo
        # self.training_session_repo = training_session_repo  # ç§»é™¤ä¼šè¯æ¦‚å¿µ
        self.knowledge_extractor = knowledge_extractor
        self.graph_service = graph_service
        self.hybrid_search_service = hybrid_search_service or HybridSearchService()
        
        self.llm = ChatOpenAI(
            api_key=settings.OPENAI_API_KEY,
            model="gpt-4o-mini",
            temperature=0.3
        )
        
        # ä½¿ç”¨ MySQL æ£€æŸ¥ç‚¹ä¿å­˜å™¨ï¼Œæ”¯æŒç‰ˆæœ¬ç®¡ç†å’Œç¼“å­˜
        from app.core.database import get_db
        self.checkpointer = MySQLCheckpointer(
            db_session_factory or get_db,
            cache_size=100,  # ç¼“å­˜100ä¸ªæ£€æŸ¥ç‚¹
            cache_ttl=300    # ç¼“å­˜5åˆ†é’Ÿ
        )
        
        self.training_graph = self._build_training_graph()
    
    def _build_training_graph(self):
        workflow = StateGraph(TrainingState)

        workflow.add_node("intent_recognition", self._recognize_intent)
        workflow.add_node("memory_search", self._search_memory)
        workflow.add_node("knowledge_extraction", self._extract_knowledge)
        workflow.add_node("context_analysis", self._analyze_context)
        workflow.add_node("question_generation", self._generate_question)
        workflow.add_node("save_message", self._save_message)

        workflow.set_entry_point("intent_recognition")

        workflow.add_edge("intent_recognition", "memory_search")

        workflow.add_conditional_edges(
            "memory_search",
            self._route_after_search,
            {
                "extract": "knowledge_extraction",
                "analyze": "context_analysis",
                "direct": "question_generation"
            }
        )

        workflow.add_edge("knowledge_extraction", "context_analysis")
        workflow.add_edge("context_analysis", "question_generation")
        workflow.add_edge("question_generation", "save_message")
        workflow.add_edge("save_message", END)

        return workflow.compile(checkpointer=self.checkpointer)
    
    def _create_event(
        self, 
        event_type: str, 
        node: str, 
        message: str, 
        result: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        event = {
            "type": event_type,
            "node": node,
            "message": message,
            "timestamp": datetime.now().isoformat()
        }
        if result is not None:
            event["result"] = result
        return event
    
    def save_graph_visualization(self, output_dir: str = "graph_visualizations"):
        """ä¿å­˜å·¥ä½œæµå›¾çš„å¯è§†åŒ–"""
        from pathlib import Path
        
        Path(output_dir).mkdir(exist_ok=True)
        graph = self.training_graph.get_graph()
        
        # 1. å°è¯•ç”Ÿæˆ PNG å›¾ç‰‡
        png_path = f"{output_dir}/training_graph.png"
        try:
            graph.draw_png(output_file_path=png_path)
            logger.info(f"âœ… å›¾å·²ä¿å­˜ä¸º PNG: {png_path}")
            return png_path
        except Exception:
            logger.debug("PNG ç”Ÿæˆå¤±è´¥ï¼Œå°è¯• Mermaid æ ¼å¼")
        
        # 2. å¤‡é€‰æ–¹æ¡ˆï¼šä¿å­˜ Mermaid
        try:
            mermaid_path = f"{output_dir}/training_graph.mmd"
            Path(mermaid_path).write_text(graph.draw_mermaid())
            logger.info(f"âœ… å›¾å·²ä¿å­˜ä¸º Mermaid: {mermaid_path}")
            logger.info("ğŸ“Š å¯åœ¨ https://mermaid.live æŸ¥çœ‹")
            return mermaid_path
        except Exception as e:
            logger.error(f"âŒ æ— æ³•ç”Ÿæˆä»»ä½•å¯è§†åŒ–: {e}")
            return None
    
    def _recognize_intent(self, state: TrainingState) -> Dict[str, Any]:
        """
        è¯†åˆ«ç”¨æˆ·æ„å›¾å¹¶åˆ¤æ–­å¯¹è¯é˜¶æ®µã€‚
        
        åˆ†æç”¨æˆ·æ¶ˆæ¯å†…å®¹ï¼Œè¯†åˆ«å…¶æ„å›¾ç±»å‹ï¼ˆå¦‚ä¿¡æ¯åˆ†äº«ã€æé—®ã€æ‰“æ‹›å‘¼ç­‰ï¼‰ï¼Œ
        å¹¶åˆ¤æ–­å½“å‰å¯¹è¯æ‰€å¤„çš„é˜¶æ®µï¼ˆåˆå§‹ã€æ¢ç´¢ã€æ·±åŒ–ã€æ€»ç»“ï¼‰ã€‚
        
        Args:
            state: å½“å‰è®­ç»ƒçŠ¶æ€ï¼Œå¿…é¡»åŒ…å« current_message å­—æ®µ
            
        Returns:
            æ›´æ–°åçš„çŠ¶æ€å­—å…¸ï¼ŒåŒ…å«ï¼š
            - intent: è¯†åˆ«å‡ºçš„æ„å›¾ç±»å‹
            - should_extract: æ˜¯å¦éœ€è¦è¿›è¡ŒçŸ¥è¯†æŠ½å–
            - conversation_stage: å½“å‰å¯¹è¯é˜¶æ®µ
            - step_results: åŒ…å«æ„å›¾è¯†åˆ«ç»“æœ
            - events: èŠ‚ç‚¹æ‰§è¡Œäº‹ä»¶åˆ—è¡¨
        """
        # èŠ‚ç‚¹å¼€å§‹äº‹ä»¶
        events = [self._create_event(
            "node_start",
            "intent_recognition",
            "ğŸ” å¼€å§‹è¯†åˆ«ç”¨æˆ·æ„å›¾..."
        )]
        
        current_step = "recognizing_intent"
        thinking_process = ["æ­£åœ¨è¯†åˆ«ç”¨æˆ·æ„å›¾..."]
        
        # æ·»åŠ æ€è€ƒæ­¥éª¤
        events.append(self._create_event(
            "thinking",
            "intent_recognition",
            "ğŸ’­ åˆ†ææ¶ˆæ¯å†…å®¹ï¼Œè¯†åˆ«ç”¨æˆ·æ„å›¾..."
        ))
        
        # æ„å»ºå¯¹è¯å†å²ä¸Šä¸‹æ–‡
        history_context = ""
        messages = state.get('messages', [])
        if len(messages) > 1:  # æœ‰å†å²æ¶ˆæ¯
            recent_messages = messages[-6:-1]  # è·å–æœ€è¿‘å‡ æ¡å†å²ï¼ˆä¸åŒ…æ‹¬å½“å‰æ¶ˆæ¯ï¼‰
            if recent_messages:
                history_context = "\nå¯¹è¯å†å²ï¼š\n"
                for msg in recent_messages:
                    # å¤„ç†æ¶ˆæ¯å¯èƒ½æ˜¯å­—å…¸æˆ–å¯¹è±¡çš„æƒ…å†µ
                    if isinstance(msg, dict):
                        role = "ç”¨æˆ·" if msg.get("role") == "user" else "åŠ©æ‰‹"
                        content = msg.get("content", "")
                    elif isinstance(msg, BaseMessage):
                        role = "ç”¨æˆ·" if isinstance(msg, UserMessage) else "åŠ©æ‰‹"
                        content = msg.content
                    else:
                        continue
                    
                    history_context += f"{role}: {content[:100]}...\n" if len(content) > 100 else f"{role}: {content}\n"
        
        prompt = f"""
åˆ†æä»¥ä¸‹ç”¨æˆ·æ¶ˆæ¯çš„æ„å›¾å’Œå†…å®¹ç±»å‹ï¼š
{history_context}
å½“å‰ç”¨æˆ·æ¶ˆæ¯: {state['current_message']}

è¯·åŸºäºå¯¹è¯å†å²å’Œå½“å‰æ¶ˆæ¯åˆ¤æ–­ï¼š
1. æ„å›¾ç±»å‹ï¼ˆinformation_sharing/question_asking/clarification/greeting/otherï¼‰
2. å½“å‰å¯¹è¯é˜¶æ®µï¼ˆinitial/exploring/deepening/concludingï¼‰

è¿”å›JSONæ ¼å¼ï¼š
{{
    "intent": "...",
    "stage": "..."
}}
"""
        
        response = self.llm.invoke([SystemMessage(content=prompt)])
        try:
            result = json.loads(response.content)
        except json.JSONDecodeError as e:
            logger.error(f"æ„å›¾è¯†åˆ«å“åº”è§£æå¤±è´¥: {e}")
            logger.error(f"åŸå§‹å“åº”: {response.content}")
            raise ValueError(f"æ„å›¾è¯†åˆ«å“åº”æ ¼å¼é”™è¯¯: {str(e)}")
        
        intent = result.get("intent", "other")
        conversation_stage = result.get("stage", "exploring")
        
        # åŸºäºæ„å›¾è®¾ç½®æ˜¯å¦éœ€è¦æå–çŸ¥è¯†
        should_extract = False
        if intent == "information_sharing":
            should_extract = True
        elif intent == "greeting":
            should_extract = False
        elif intent == "question_asking":
            # é—®é¢˜å¯èƒ½åŒ…å«çŸ¥è¯†ï¼Œä¹Ÿå¯èƒ½ä¸åŒ…å«
            # è¿™é‡Œç®€å•å¤„ç†ï¼Œå¦‚æœæ˜¯æ¢ç´¢é˜¶æ®µçš„é—®é¢˜ï¼Œå¯èƒ½æœ‰çŸ¥è¯†
            should_extract = conversation_stage in ["exploring", "deepening"]
        elif intent == "other":
            # å¯¹äº other ç±»å‹ï¼ŒåŸºäºæ–‡æœ¬é•¿åº¦å’Œå†…å®¹å¯†åº¦åˆ¤æ–­
            # é•¿æ–‡æœ¬ï¼ˆè¶…è¿‡100å­—ç¬¦ï¼‰æˆ–åŒ…å«ä¸“ä¸šæœ¯è¯­çš„æ–‡æœ¬åº”è¯¥æå–
            text_length = len(state['current_message'])
            if text_length > 100:
                should_extract = True
                logger.debug(f"é•¿æ–‡æœ¬({text_length}å­—ç¬¦)è¢«è¯†åˆ«ä¸ºéœ€è¦æå–çŸ¥è¯†")
            else:
                should_extract = False
        else:
            should_extract = False
        
        # æ„å›¾å­˜å‚¨åœ¨ step_results ä¸­ï¼Œä¸æ±¡æŸ“é¡¶çº§ state
        step_results = state.get('step_results', {}).copy()
        step_results["intent_recognition"] = {
            "intent": intent,
            "should_extract": should_extract,
            "stage": conversation_stage
        }
        
        # èŠ‚ç‚¹å®Œæˆäº‹ä»¶
        events.append(self._create_event(
            "node_complete",
            "intent_recognition",
            f"âœ… æ„å›¾è¯†åˆ«å®Œæˆ: {intent}",
            {
                "intent": intent,
                "stage": conversation_stage,
                "should_extract": should_extract
            }
        ))
        
        completed_steps = ["intent_recognition"]
        thinking_process.append(f"è¯†åˆ«åˆ°æ„å›¾: {intent}, å¯¹è¯é˜¶æ®µ: {conversation_stage}")
        
        # è¿”å›æ›´æ–°çš„å­—æ®µ
        return {
            "current_step": current_step,
            "conversation_stage": conversation_stage,
            "should_extract": should_extract,
            "step_results": step_results,
            "completed_steps": completed_steps,
            "thinking_process": thinking_process,
            "events": events
        }
    
    def _route_after_intent(self, state: TrainingState) -> str:
        if state.get('should_extract', False):
            return "extract"
        elif state.get('total_knowledge_points', 0) > 5:
            return "analyze"
        else:
            return "direct"

    async def _search_memory(self, state: TrainingState) -> Dict[str, Any]:
        """
        æœç´¢ç›¸å…³è®°å¿†ä»¥æä¾›ä¸Šä¸‹æ–‡ã€‚

        åŸºäºå½“å‰æ¶ˆæ¯å†…å®¹ï¼Œæœç´¢æ•°å­—äººçš„çŸ¥è¯†åº“ä¸­ç›¸å…³çš„å®ä½“ã€å…³ç³»å’Œæ–‡æœ¬å—ï¼Œ
        ä¸ºåç»­çš„ç†è§£å’Œå›åº”æä¾›ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚

        Args:
            state: å½“å‰è®­ç»ƒçŠ¶æ€ï¼ŒåŒ…å« current_message å’Œ digital_human_id

        Returns:
            æ›´æ–°åçš„çŠ¶æ€å­—å…¸ï¼ŒåŒ…å«ï¼š
            - memory_search_results: æœç´¢åˆ°çš„ç›¸å…³è®°å¿†
            - events: èŠ‚ç‚¹æ‰§è¡Œäº‹ä»¶åˆ—è¡¨
        """
        events = [self._create_event(
            "node_start",
            "memory_search",
            "ğŸ” æœç´¢ç›¸å…³è®°å¿†..."
        )]

        current_step = "searching_memory"
        thinking_process = ["æ­£åœ¨æœç´¢ç›¸å…³è®°å¿†..."]

        try:
            # ä½¿ç”¨æ··åˆæœç´¢è·å–ç›¸å…³è®°å¿†
            search_results = await self.hybrid_search_service.search(
                query=state['current_message'],
                digital_human_id=state['digital_human_id'],
                mode="hybrid",
                entity_limit=5,
                relationship_limit=3,
                expand_graph=True
            )

            # è®°å½•æœç´¢ç»Ÿè®¡
            stats = search_results.get("statistics", {})
            events.append(self._create_event(
                "memory_found",
                "memory_search",
                f"âœ… æ‰¾åˆ° {stats.get('total_entities', 0)} ä¸ªç›¸å…³å®ä½“ï¼Œ{stats.get('total_relationships', 0)} ä¸ªå…³ç³»",
                {
                    "entities_count": stats.get('total_entities', 0),
                    "relationships_count": stats.get('total_relationships', 0)
                }
            ))

            thinking_process.append(f"æ‰¾åˆ° {stats.get('total_entities', 0)} ä¸ªç›¸å…³è®°å¿†ç‚¹")

        except Exception as e:
            logger.warning(f"è®°å¿†æœç´¢å¤±è´¥: {e}")
            search_results = {
                "entities": [],
                "relationships": [],
                "statistics": {"total_entities": 0, "total_relationships": 0}
            }
            events.append(self._create_event(
                "memory_search_failed",
                "memory_search",
                "âš ï¸ è®°å¿†æœç´¢å¤±è´¥ï¼Œç»§ç»­å¤„ç†"
            ))

        events.append(self._create_event(
            "node_complete",
            "memory_search",
            "âœ… è®°å¿†æœç´¢å®Œæˆ"
        ))

        return {
            "current_step": current_step,
            "memory_search_results": search_results,
            "thinking_process": thinking_process,
            "events": events
        }

    def _route_after_search(self, state: TrainingState) -> str:
        """åŸºäºæœç´¢ç»“æœå†³å®šä¸‹ä¸€æ­¥è·¯ç”±"""
        # å¦‚æœåŸæœ¬å°±éœ€è¦æå–çŸ¥è¯†ï¼Œç»§ç»­æå–
        if state.get('should_extract', False):
            return "extract"
        # å¦‚æœæ‰¾åˆ°äº†ç›¸å…³è®°å¿†ï¼Œè¿›è¡Œä¸Šä¸‹æ–‡åˆ†æ
        elif state.get('memory_search_results', {}).get('statistics', {}).get('total_entities', 0) > 0:
            return "analyze"
        # å¦‚æœå·²æœ‰è¶³å¤Ÿçš„çŸ¥è¯†ç‚¹ï¼Œè¿›è¡Œåˆ†æ
        elif state.get('total_knowledge_points', 0) > 5:
            return "analyze"
        # å¦åˆ™ç›´æ¥ç”Ÿæˆé—®é¢˜
        else:
            return "direct"

    async def _extract_knowledge(self, state: TrainingState) -> Dict[str, Any]:
        """
        ä»ç”¨æˆ·æ¶ˆæ¯ä¸­æå–çŸ¥è¯†å®ä½“å’Œå…³ç³»ã€‚
        
        ä½¿ç”¨çŸ¥è¯†æŠ½å–å™¨åˆ†ææ–‡æœ¬ï¼Œè¯†åˆ«å®ä½“ï¼ˆäººç‰©ã€ç»„ç»‡ã€æŠ€èƒ½ç­‰ï¼‰
        åŠå…¶ä¹‹é—´çš„å…³ç³»ï¼Œå¹¶å°†ç»“æœå­˜å‚¨åˆ°å›¾æ•°æ®åº“ä¸­ã€‚
        
        Args:
            state: å½“å‰è®­ç»ƒçŠ¶æ€ï¼Œå¿…é¡»åŒ…å« current_message å’Œ digital_human_id
            
        Returns:
            æ›´æ–°åçš„çŠ¶æ€å­—å…¸ï¼ŒåŒ…å«ï¼š
            - extracted_knowledge: æå–çš„å®ä½“å’Œå…³ç³»
            - step_results: åŒ…å«çŸ¥è¯†æŠ½å–ç»“æœç»Ÿè®¡
            - events: èŠ‚ç‚¹æ‰§è¡Œäº‹ä»¶åˆ—è¡¨
        """
        events = [self._create_event(
            "node_start",
            "knowledge_extraction",
            "ğŸ§  å¼€å§‹æå–çŸ¥è¯†ç‚¹..."
        )]
        
        current_step = "extracting_knowledge"
        thinking_process = ["æ­£åœ¨æå–çŸ¥è¯†ç‚¹..."]
        
        if not state.get('should_extract', False):
            extracted_knowledge = {"entities": [], "relationships": []}
            completed_steps = ["knowledge_extraction"]
            
            events.append(self._create_event(
                "node_complete",
                "knowledge_extraction",
                "â„¹ï¸ æ— éœ€æå–çŸ¥è¯†"
            ))
            
            return {
                "current_step": current_step,
                "extracted_knowledge": extracted_knowledge,
                "completed_steps": completed_steps,
                "thinking_process": thinking_process,
                "events": events
            }
        
        # ä½¿ç”¨ extract_with_embeddings ç”Ÿæˆå‘é‡åµŒå…¥
        extraction_result = await self.knowledge_extractor.extract_with_embeddings(
            state['current_message'],
            state['digital_human_id']
        )
        extracted_knowledge = extraction_result

        entity_count = len(extraction_result.get("entities", []))
        relationship_count = len(extraction_result.get("relationships", []))

        # å­˜å‚¨å®ä½“åˆ°å›¾æ•°æ®åº“ï¼ˆç°åœ¨åŒ…å«embedding_idï¼‰
        for entity in extraction_result.get("entities", []):
            await self.graph_service.store_digital_human_entity(state['digital_human_id'], entity)

        # å­˜å‚¨å…³ç³»åˆ°å›¾æ•°æ®åº“ï¼ˆç°åœ¨åŒ…å«embedding_idï¼‰
        for relationship in extraction_result.get("relationships", []):
            await self.graph_service.store_digital_human_relationship(state['digital_human_id'], relationship)
        
        step_results = state.get('step_results', {}).copy()
        step_results["knowledge_extraction"] = {
            "entities_count": entity_count,
            "relationships_count": relationship_count,
            "extracted": extraction_result
        }
        
        events.append(self._create_event(
            "node_complete",
            "knowledge_extraction",
            f"âœ… çŸ¥è¯†æå–å®Œæˆ: {entity_count} ä¸ªå®ä½“, {relationship_count} ä¸ªå…³ç³»",
            {
                "entities_count": entity_count,
                "relationships_count": relationship_count
            }
        ))
        
        completed_steps = ["knowledge_extraction"]
        thinking_process.append(f"æå–åˆ° {entity_count} ä¸ªå®ä½“, {relationship_count} ä¸ªå…³ç³»")
        
        return {
            "current_step": current_step,
            "extracted_knowledge": extracted_knowledge,
            "step_results": step_results,
            "completed_steps": completed_steps,
            "thinking_process": thinking_process,
            "events": events
        }
    
    def _analyze_context(self, state: TrainingState) -> Dict[str, Any]:
        """
        åˆ†ææ•°å­—äººå·²æœ‰çš„çŸ¥è¯†ä¸Šä¸‹æ–‡ã€‚
        
        ä»å›¾æ•°æ®åº“è·å–æ•°å­—äººå·²å­˜å‚¨çš„çŸ¥è¯†ç‚¹ï¼Œåˆ†æå…¶åˆ†å¸ƒæƒ…å†µï¼Œ
        åˆ¤æ–­æ˜¯å¦éœ€è¦æ·±å…¥æ¢ç´¢æŸäº›é¢†åŸŸã€‚
        
        Args:
            state: å½“å‰è®­ç»ƒçŠ¶æ€ï¼Œå¿…é¡»åŒ…å« digital_human_id
            
        Returns:
            æ›´æ–°åçš„çŠ¶æ€å­—å…¸ï¼ŒåŒ…å«ï¼š
            - knowledge_context: çŸ¥è¯†ä¸Šä¸‹æ–‡ä¿¡æ¯
            - total_knowledge_points: æ€»çŸ¥è¯†ç‚¹æ•°
            - categories: çŸ¥è¯†åˆ†ç±»ç»Ÿè®¡
            - should_explore_deeper: æ˜¯å¦éœ€è¦æ·±å…¥æ¢ç´¢
            - events: èŠ‚ç‚¹æ‰§è¡Œäº‹ä»¶åˆ—è¡¨
        """
        events = [self._create_event(
            "node_start",
            "context_analysis",
            "ğŸ” å¼€å§‹åˆ†æçŸ¥è¯†å›¾è°±ä¸Šä¸‹æ–‡..."
        )]
        
        current_step = "analyzing_context"
        thinking_process = ["æ­£åœ¨åˆ†æçŸ¥è¯†å›¾è°±ä¸Šä¸‹æ–‡..."]
        
        context = self._get_current_context(state['digital_human_id'])
        knowledge_context = context
        total_knowledge_points = context.get("total_knowledge_points", 0)
        categories = context.get("categories", {})
        
        should_explore_deeper = False
        if total_knowledge_points > 20 and not categories.get("hobby"):
            should_explore_deeper = True
        elif total_knowledge_points > 10 and len(categories) < 3:
            should_explore_deeper = True
        
        step_results = state.get('step_results', {}).copy()
        step_results["context_analysis"] = {
            "total_points": total_knowledge_points,
            "categories_count": len(categories),
            "should_explore_deeper": should_explore_deeper
        }
        
        events.append(self._create_event(
            "node_complete",
            "context_analysis",
            f"âœ… ä¸Šä¸‹æ–‡åˆ†æå®Œæˆ: {total_knowledge_points} ä¸ªçŸ¥è¯†ç‚¹",
            {
                "total_points": total_knowledge_points,
                "categories_count": len(categories)
            }
        ))
        
        completed_steps = ["context_analysis"]
        thinking_process.append(f"å·²äº†è§£ {total_knowledge_points} ä¸ªçŸ¥è¯†ç‚¹ï¼Œæ¶µç›– {len(categories)} ä¸ªé¢†åŸŸ")
        
        return {
            "current_step": current_step,
            "knowledge_context": knowledge_context,
            "total_knowledge_points": total_knowledge_points,
            "categories": categories,
            "should_explore_deeper": should_explore_deeper,
            "step_results": step_results,
            "completed_steps": completed_steps,
            "thinking_process": thinking_process,
            "events": events
        }
    
    def _generate_question(self, state: TrainingState) -> Dict[str, Any]:
        """
        åŸºäºå½“å‰çŠ¶æ€ç”Ÿæˆå¼•å¯¼æ€§é—®é¢˜ã€‚
        
        æ ¹æ®ç”¨æˆ·çš„å›ç­”ã€å·²æå–çš„çŸ¥è¯†å’Œå¯¹è¯é˜¶æ®µï¼Œ
        ç”Ÿæˆä¸‹ä¸€ä¸ªè‡ªç„¶çš„å¼•å¯¼æ€§é—®é¢˜ï¼Œæ¨åŠ¨å¯¹è¯æ·±å…¥ã€‚
        
        Args:
            state: å½“å‰è®­ç»ƒçŠ¶æ€ï¼ŒåŒ…å«å¯¹è¯å†å²å’ŒçŸ¥è¯†ä¸Šä¸‹æ–‡
            
        Returns:
            æ›´æ–°åçš„çŠ¶æ€å­—å…¸ï¼ŒåŒ…å«ï¼š
            - next_question: ç”Ÿæˆçš„å¼•å¯¼æ€§é—®é¢˜
            - step_results: åŒ…å«é—®é¢˜ç”Ÿæˆç›¸å…³ä¿¡æ¯
            - events: èŠ‚ç‚¹æ‰§è¡Œäº‹ä»¶åˆ—è¡¨
        """
        events = [self._create_event(
            "node_start",
            "question_generation",
            "â“ å¼€å§‹ç”Ÿæˆå¼•å¯¼æ€§é—®é¢˜..."
        )]
        
        current_step = "generating_question"
        thinking_process = ["æ­£åœ¨ç”Ÿæˆå¼•å¯¼æ€§é—®é¢˜..."]
        
        context_prompt = self._build_context_prompt(state)
        
        # æ„å»ºåŒ…å«å†å²çš„æ¶ˆæ¯åˆ—è¡¨
        messages = []
        
        # æ·»åŠ ç³»ç»Ÿæç¤º
        system_prompt = f"""
ä½ æ˜¯ä¸€ä¸ªæ­£åœ¨äº†è§£ç”¨æˆ·çš„æ•°å­—äººã€‚åŸºäºå½“å‰å¯¹è¯çŠ¶æ€ï¼Œç”Ÿæˆä¸‹ä¸€ä¸ªå¼•å¯¼æ€§é—®é¢˜ã€‚

{context_prompt}

è¦æ±‚ï¼š
1. é—®é¢˜è¦è‡ªç„¶ã€å‹å¥½
2. æ ¹æ®ç”¨æˆ·åˆšæ‰çš„å›ç­”å»¶ä¼¸
3. é€æ­¥æ·±å…¥äº†è§£ç”¨æˆ·
4. ä¸è¦é‡å¤å·²ç»é—®è¿‡çš„å†…å®¹
5. åŸºäºå¯¹è¯å†å²ä¿æŒè¿è´¯æ€§

ç”Ÿæˆä¸€ä¸ªå¼•å¯¼æ€§é—®é¢˜ï¼š
"""
        messages.append(SystemMessage(content=system_prompt))
        
        # æ·»åŠ å†å²æ¶ˆæ¯ï¼ˆä¿ç•™æœ€è¿‘10æ¡ï¼‰
        recent_messages = state.get('messages', [])[-10:]
        if recent_messages:
            logger.debug(f"ğŸ’¬ åŒ…å« {len(recent_messages)} æ¡å†å²æ¶ˆæ¯")
            
            # å¤„ç†æ¶ˆæ¯æ ¼å¼ï¼Œç¡®ä¿éƒ½æ˜¯ BaseMessage å¯¹è±¡
            for msg in recent_messages:
                if isinstance(msg, BaseMessage):
                    messages.append(msg)
                elif isinstance(msg, dict):
                    # å°†å­—å…¸æ ¼å¼è½¬æ¢ä¸º BaseMessage å¯¹è±¡
                    role = msg.get("role", "")
                    content = msg.get("content", "")
                    if role == "user" or role == "human":
                        messages.append(UserMessage(content=content))
                    elif role == "assistant" or role == "ai":
                        messages.append(AssistantMessage(content=content))
                    elif role == "system":
                        messages.append(SystemMessage(content=content))
                    else:
                        logger.warning(f"âš ï¸ æœªçŸ¥æ¶ˆæ¯è§’è‰²: {role}")
            
            # æ‰“å°æ¶ˆæ¯å†…å®¹æ‘˜è¦ç”¨äºè°ƒè¯•
            for idx, msg in enumerate(messages[-3:]):  # åªæ‰“å°æœ€è¿‘3æ¡
                content_preview = msg.content[:50] + "..." if len(msg.content) > 50 else msg.content
                logger.debug(f"  æ¶ˆæ¯{idx+1}: [{msg.__class__.__name__}] {content_preview}")
        
        # æ‰“å°å®Œæ•´çš„æ¶ˆæ¯åˆ—è¡¨ä¼ é€’ç»™LLMï¼ˆä¸´æ—¶è°ƒè¯•ï¼‰
        logger.info(f"ğŸ” ä¼ é€’ç»™LLMçš„å®Œæ•´æ¶ˆæ¯åˆ—è¡¨ï¼ˆå…±{len(messages)}æ¡ï¼‰ï¼š")
        for idx, msg in enumerate(messages):
            content_preview = msg.content[:100] + "..." if len(msg.content) > 100 else msg.content
            logger.info(f"  [{idx}] {msg.__class__.__name__}: {content_preview}")
        
        response = self.llm.invoke(messages)
        next_question = response.content.strip()
        
        step_results = state.get('step_results', {}).copy()
        step_results["question_generation"] = {
            "question": next_question,
            "based_on_stage": state.get('conversation_stage', 'exploring')
        }
        
        events.append(self._create_event(
            "node_complete",
            "question_generation",
            "âœ… é—®é¢˜ç”Ÿæˆå®Œæˆ",
            {
                "question": next_question[:50] + "..." if len(next_question) > 50 else next_question
            }
        ))
        
        completed_steps = ["question_generation"]
        thinking_process.append("å·²ç”Ÿæˆå¼•å¯¼æ€§é—®é¢˜")
        
        # å°†åŠ©æ‰‹å›å¤æ·»åŠ åˆ°æ¶ˆæ¯å†å²ï¼ˆä½¿ç”¨å¯åºåˆ—åŒ–çš„å­—å…¸æ ¼å¼ï¼‰
        messages = [{
            "role": "assistant",
            "content": next_question,
            "additional_kwargs": {}
        }]
        
        return {
            "current_step": current_step,
            "next_question": next_question,
            "step_results": step_results,
            "completed_steps": completed_steps,
            "thinking_process": thinking_process,
            "events": events,
            "messages": messages  # è¿½åŠ åŠ©æ‰‹æ¶ˆæ¯åˆ°å†å²
        }
    
    def _build_context_prompt(self, state: Dict[str, Any]) -> str:
        prompt_parts = []

        if state.get('current_message'):
            prompt_parts.append(f"ç”¨æˆ·åˆšæ‰è¯´: {state['current_message']}")

        # æ·»åŠ è®°å¿†æœç´¢ç»“æœ
        memory_search_results = state.get('memory_search_results', {})
        if memory_search_results:
            memory_entities = memory_search_results.get('entities', [])
            if memory_entities:
                entity_summaries = []
                for entity in memory_entities[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ªæœ€ç›¸å…³çš„
                    name = entity.get('name', '')
                    desc = entity.get('description', '')
                    if name:
                        summary = f"{name}"
                        if desc:
                            summary += f" ({desc[:50]}...)" if len(desc) > 50 else f" ({desc})"
                        entity_summaries.append(summary)
                if entity_summaries:
                    prompt_parts.append(f"ç›¸å…³è®°å¿†: {'; '.join(entity_summaries)}")

            memory_relationships = memory_search_results.get('relationships', [])
            if memory_relationships:
                rel_summaries = []
                for rel in memory_relationships[:2]:  # åªæ˜¾ç¤ºå‰2ä¸ªå…³ç³»
                    source = rel.get('source', '')
                    target = rel.get('target', '')
                    types = rel.get('types', [])
                    if source and target:
                        type_str = types[0] if types else "ç›¸å…³"
                        rel_summaries.append(f"{source} -> {target} ({type_str})")
                if rel_summaries:
                    prompt_parts.append(f"è®°å¿†ä¸­çš„å…³ç³»: {'; '.join(rel_summaries)}")

        extracted_knowledge = state.get('extracted_knowledge', {})
        if extracted_knowledge and extracted_knowledge.get("entities"):
            entities = extracted_knowledge["entities"]
            entity_names = [e.get("name") for e in entities[:3]]
            prompt_parts.append(f"æå–åˆ°çš„å®ä½“: {', '.join(entity_names)}")

        total_knowledge_points = state.get('total_knowledge_points', 0)
        if total_knowledge_points > 0:
            prompt_parts.append(f"å·²äº†è§£çš„çŸ¥è¯†ç‚¹æ•°: {total_knowledge_points}")

        categories = state.get('categories', {})
        if categories:
            cat_summary = []
            for cat, info in categories.items():
                if isinstance(info, dict) and info.get("count"):
                    cat_summary.append(f"{cat}({info['count']}ä¸ª)")
            if cat_summary:
                prompt_parts.append(f"å·²äº†è§£çš„é¢†åŸŸ: {', '.join(cat_summary)}")

        conversation_stage = state.get('conversation_stage', 'exploring')
        prompt_parts.append(f"å½“å‰å¯¹è¯é˜¶æ®µ: {conversation_stage}")

        return "\n".join(prompt_parts)
    
    async def _save_message(self, state: TrainingState) -> Dict[str, Any]:
        """
        ä¿å­˜æ¶ˆæ¯åˆ°æ•°æ®åº“ã€‚
        
        å°†ç”¨æˆ·æ¶ˆæ¯å’ŒåŠ©æ‰‹å›å¤æŒä¹…åŒ–åˆ°æ•°æ®åº“ä¸­ï¼Œ
        ç”¨äºåç»­çš„å¯¹è¯å†å²æŸ¥è¯¢å’Œåˆ†æã€‚
        
        Args:
            state: å½“å‰è®­ç»ƒçŠ¶æ€ï¼Œå¿…é¡»åŒ…å« digital_human_id, user_id å’Œ current_message
            
        Returns:
            æ›´æ–°åçš„çŠ¶æ€å­—å…¸ï¼ŒåŒ…å«ï¼š
            - messages: æ›´æ–°åçš„æ¶ˆæ¯åˆ—è¡¨ï¼ˆè¿½åŠ æ–°æ¶ˆæ¯ï¼‰
            - step_results: åŒ…å«æ¶ˆæ¯ä¿å­˜ç»“æœ
            - events: èŠ‚ç‚¹æ‰§è¡Œäº‹ä»¶åˆ—è¡¨
        """
        events = [self._create_event(
            "node_start",
            "save_message",
            "ğŸ’¾ å¼€å§‹ä¿å­˜å¯¹è¯è®°å½•..."
        )]
        
        current_step = "saving_message"
        thinking_process = ["æ­£åœ¨ä¿å­˜å¯¹è¯è®°å½•..."]
        
        message_data = {
            "digital_human_id": state['digital_human_id'],
            "user_id": state['user_id'],
            "content": state['current_message'],
            "extracted_knowledge": state.get('extracted_knowledge', {}),
            "conversation_stage": state.get('conversation_stage', 'exploring'),
            "next_question": state.get('next_question', '')
        }
        
        step_results = state.get('step_results', {}).copy()
        step_results["message_saving"] = {
            "saved": True,
            "message_length": len(state['current_message'])
        }
        
        events.append(self._create_event(
            "node_complete",
            "save_message",
            "âœ… å¯¹è¯è®°å½•ä¿å­˜å®Œæˆ"
        ))
        
        completed_steps = ["save_message"]
        thinking_process.append("å¯¹è¯è®°å½•å·²ä¿å­˜")
        
        return {
            "current_step": current_step,
            "step_results": step_results,
            "completed_steps": completed_steps,
            "thinking_process": thinking_process,
            "events": events
        }
    
    def _get_current_context(self, digital_human_id: int) -> Dict[str, Any]:
        try:
            return self.graph_service.get_digital_human_knowledge_context(digital_human_id)
        except Exception as e:
            logger.error(f"è·å–è®­ç»ƒä¸Šä¸‹æ–‡å¤±è´¥: {str(e)}")
            return {"total_knowledge_points": 0, "categories": {}, "recent_entities": []}
    
    async def _save_and_send_assistant_message(
        self,
        digital_human_id: int,
        user_id: int,
        question: str
    ) -> AsyncGenerator[str, None]:
        """ä¿å­˜åŠ©æ‰‹æ¶ˆæ¯å¹¶å‘é€äº‹ä»¶"""
        assistant_msg = self.training_message_repo.create_training_message(
            digital_human_id=digital_human_id,
            user_id=user_id,
            role="assistant",
            content=question
        )
        
        yield json.dumps({
            "type": "assistant_question",
            "id": assistant_msg.id,
            "data": question
        }, ensure_ascii=False)
    
    def _extract_next_question(self, result) -> Optional[str]:
        """ä»ç»“æœä¸­æå–ä¸‹ä¸€ä¸ªé—®é¢˜"""
        if not result:
            return None
            
        # å°è¯•ä½œä¸ºå­—å…¸è®¿é—®
        if hasattr(result, '__getitem__') and 'next_question' in result:
            return result['next_question']
        # å°è¯•ä½œä¸ºå¯¹è±¡å±æ€§è®¿é—®
        elif hasattr(result, 'next_question'):
            return result.next_question
            
        return None
    
    def get_training_history(
        self,
        digital_human_id: int,
        page: int = 1,
        size: int = 20
    ) -> Tuple[List[DigitalHumanTrainingMessage], int]:
        """è·å–è®­ç»ƒå†å²è®°å½•"""
        return self.training_message_repo.get_training_messages_paginated(
            digital_human_id=digital_human_id,
            page=page,
            size=size
        )
    
    async def process_training_conversation(
        self,
        digital_human_id: int,
        user_message: str,
        user_id: int
    ) -> Generator[str, None, None]:
        user_msg = None
        state = None
        
        try:
            # ä½¿ç”¨å›ºå®šçš„ thread_idï¼Œç¡®ä¿è®­ç»ƒçš„è¿ç»­æ€§
            thread_id = f"training_{digital_human_id}_{user_id}"
            
            # ä¿å­˜ç”¨æˆ·æ¶ˆæ¯åˆ°è®­ç»ƒæ¶ˆæ¯è¡¨
            user_msg = self.training_message_repo.create_training_message(
                digital_human_id=digital_human_id,
                user_id=user_id,
                role="user",
                content=user_message
            )
            
            # ç¡®ä¿ id æ˜¯å¯åºåˆ—åŒ–çš„
            msg_id = user_msg.id if hasattr(user_msg, 'id') else None
            
            # å¦‚æœæ˜¯ Mock å¯¹è±¡ï¼Œè·å–å…¶å®é™…å€¼
            if hasattr(msg_id, '_mock_name'):
                msg_id = 100  # é»˜è®¤å€¼
                
            yield json.dumps({
                "type": "user_message",
                "id": msg_id,
                "data": user_message
            }, ensure_ascii=False)
            
            # é…ç½® thread_id ç”¨äº checkpointer
            config = {"configurable": {"thread_id": thread_id}}
            
            # ä» checkpointer åŠ è½½å†å²æ¶ˆæ¯
            existing_messages = []
            try:
                checkpoint = self.checkpointer.get(config["configurable"])
                if checkpoint and checkpoint.get("channel_values"):
                    existing_messages = checkpoint["channel_values"].get("messages", [])
                    logger.debug(f"ğŸ“š åŠ è½½å†å²æ¶ˆæ¯: {len(existing_messages)} æ¡")
                    
                    # æ‰“å°æœ€è¿‘å‡ æ¡å†å²æ¶ˆæ¯ç”¨äºè°ƒè¯•
                    for idx, msg in enumerate(existing_messages[-3:]):
                        if isinstance(msg, BaseMessage):
                            content_preview = msg.content[:50] + "..." if len(msg.content) > 50 else msg.content
                            logger.debug(f"  å†å²{idx+1}: [{msg.__class__.__name__}] {content_preview}")
                        elif isinstance(msg, dict):
                            content_preview = msg.get("content", "")[:50] + "..." if len(msg.get("content", "")) > 50 else msg.get("content", "")
                            logger.debug(f"  å†å²{idx+1}: [dict:{msg.get('role', 'unknown')}] {content_preview}")
            except Exception as e:
                logger.warning(f"âš ï¸ åŠ è½½å†å²æ¶ˆæ¯å¤±è´¥: {str(e)}")
                existing_messages = []
            
            # æ·»åŠ å½“å‰æ¶ˆæ¯åˆ°å†å²ï¼ˆä½¿ç”¨å­—å…¸æ ¼å¼ä¿æŒä¸€è‡´æ€§ï¼‰
            existing_messages.append({
                "role": "user",
                "content": user_message,
                "additional_kwargs": {}
            })
            logger.debug(f"ğŸ“ æ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯: {user_message[:50]}...")
            
            # è·å–å½“å‰ä¸Šä¸‹æ–‡
            knowledge_context = self._get_current_context(digital_human_id)
            
            state = TrainingState(
                digital_human_id=digital_human_id,
                user_id=user_id,
                current_message=user_message,
                messages=existing_messages,  # ä½¿ç”¨ä» checkpointer åŠ è½½çš„å†å²æ¶ˆæ¯
                extracted_knowledge={},
                knowledge_context=knowledge_context,
                next_question="",
                should_extract=False,
                should_explore_deeper=False,
                conversation_stage="exploring",
                total_knowledge_points=knowledge_context.get("total_knowledge_points", 0),
                categories=knowledge_context.get("categories", {}),
                current_step="initializing",
                completed_steps=[],
                step_results={},
                thinking_process=[],
                events=[]
            )
            
            yield json.dumps({
                "type": "thinking",
                "data": "å¼€å§‹åˆ†æå¯¹è¯..."
            }, ensure_ascii=False)
            
            # ä¿å­˜æœ€ç»ˆçŠ¶æ€
            final_state = None
            previous_state = None
            
            # ä½¿ç”¨æµå¼è·å–çŠ¶æ€æ›´æ–°ï¼Œä¼ å…¥ config ä»¥å¯ç”¨ checkpointer
            # å¤„ç†æµå¼è¾“å‡º
            async for chunk in self.training_graph.astream(state, config):
                # chunk æ˜¯ {"èŠ‚ç‚¹å": èŠ‚ç‚¹çŠ¶æ€} æ ¼å¼
                if chunk and isinstance(chunk, dict):
                    logger.debug(f"ğŸ“Š çŠ¶æ€æ›´æ–°: {list(chunk.keys())}")
                    
                    # å¤„ç†æ¯ä¸ªèŠ‚ç‚¹çš„è¾“å‡º
                    for node_name, node_state in chunk.items():
                        # è·³è¿‡å†…éƒ¨èŠ‚ç‚¹
                        if node_name.startswith('__'):
                            continue
                            
                        # èŠ‚ç‚¹çŠ¶æ€å¯èƒ½æ˜¯ dict æˆ–å¯¹è±¡
                        if isinstance(node_state, dict):
                            # ä» dict ä¸­æå–å­—æ®µ
                            events = node_state.get('events', [])
                            completed_steps = node_state.get('completed_steps', [])
                            thinking_process = node_state.get('thinking_process', [])
                            extracted_knowledge = node_state.get('extracted_knowledge', {})
                            next_question = node_state.get('next_question', '')
                            conversation_stage = node_state.get('conversation_stage', '')
                        else:
                            # ä»å¯¹è±¡ä¸­æå–å­—æ®µ
                            events = getattr(node_state, 'events', [])
                            completed_steps = getattr(node_state, 'completed_steps', [])
                            thinking_process = getattr(node_state, 'thinking_process', [])
                            extracted_knowledge = getattr(node_state, 'extracted_knowledge', {})
                            next_question = getattr(node_state, 'next_question', '')
                            conversation_stage = getattr(node_state, 'conversation_stage', '')
                            
                        # å‘é€èŠ‚ç‚¹äº‹ä»¶
                        if events:
                            for event in events:
                                # å‘é€èŠ‚ç‚¹å†…éƒ¨çš„äº‹ä»¶
                                logger.debug(f"ğŸ“¨ å‘é€äº‹ä»¶: {event.get('type')} - {event.get('node')}")
                                yield json.dumps(event, ensure_ascii=False)
                        
                        # æ£€æµ‹æ–°å®Œæˆçš„æ­¥éª¤
                        if completed_steps:
                            if previous_state:
                                prev_completed = previous_state.get('completed_steps', []) if isinstance(previous_state, dict) else getattr(previous_state, 'completed_steps', [])
                                # æ‰¾å‡ºæ–°å®Œæˆçš„æ­¥éª¤
                                new_steps = set(completed_steps) - set(prev_completed)
                                for step in new_steps:
                                    # å¦‚æœäº‹ä»¶ä¸­æ²¡æœ‰åŒ…å«ï¼Œæ‰å‘é€
                                    if not any(e.get('type') == 'node_complete' and e.get('node') == step for e in events):
                                        yield json.dumps(
                                            self._create_event(
                                                "node_complete",
                                                step,
                                                f"âœ… å®Œæˆ: {step}"
                                            ),
                                            ensure_ascii=False
                                        )
                            
                        # æ£€æŸ¥æ€è€ƒè¿‡ç¨‹
                        if thinking_process:
                            # å‘é€æ–°çš„æ€è€ƒè¿‡ç¨‹
                            if previous_state:
                                prev_thinking = previous_state.get('thinking_process', []) if isinstance(previous_state, dict) else getattr(previous_state, 'thinking_process', [])
                                prev_count = len(prev_thinking)
                                new_thoughts = thinking_process[prev_count:]
                                for thought in new_thoughts:
                                    # thinking äº‹ä»¶æ²¡æœ‰ node å±æ€§ï¼Œä½¿ç”¨ç®€åŒ–æ ¼å¼
                                    yield json.dumps({
                                        "type": "thinking",
                                        "data": thought
                                    }, ensure_ascii=False)
                            else:
                                for thought in thinking_process:
                                    # thinking äº‹ä»¶æ²¡æœ‰ node å±æ€§ï¼Œä½¿ç”¨ç®€åŒ–æ ¼å¼
                                    yield json.dumps({
                                        "type": "thinking",
                                        "data": thought
                                    }, ensure_ascii=False)
                        
                        # æ£€æŸ¥çŸ¥è¯†æå–
                        if extracted_knowledge and extracted_knowledge.get('entities'):
                            user_msg.extracted_knowledge = extracted_knowledge
                            user_msg.extraction_metadata = {
                                "extraction_time": datetime.now().isoformat(),
                                "stage": conversation_stage
                            }
                            yield json.dumps({
                                "type": "knowledge_extracted",
                                "id": user_msg.id,
                                "data": extracted_knowledge['entities']
                            }, ensure_ascii=False)
                        
                        # æ£€æŸ¥ä¸‹ä¸€ä¸ªé—®é¢˜
                        if next_question:
                            final_state = node_state
                            logger.info(f"âœ¨ æ‰¾åˆ°ä¸‹ä¸€ä¸ªé—®é¢˜: {next_question[:50]}...")
                        
                        # ä¿å­˜å½“å‰çŠ¶æ€
                        previous_state = node_state
                    
            
            # åœ¨æµç»“æŸåï¼Œæ£€æŸ¥æ˜¯å¦æœ‰æœ€ç»ˆçŠ¶æ€
            if final_state:
                next_q = final_state.get('next_question') if isinstance(final_state, dict) else getattr(final_state, 'next_question', None)
                if next_q:
                    logger.info(f"ğŸ¤– ä»æœ€ç»ˆçŠ¶æ€æå–é—®é¢˜: {next_q}")
                    async for msg in self._save_and_send_assistant_message(
                        digital_human_id, user_id, next_q
                    ):
                        yield msg
            else:
                # å¦‚æœæ²¡æœ‰ä»æµä¸­è·å–åˆ°çŠ¶æ€ï¼Œå°è¯•ç›´æ¥è¿è¡Œ
                logger.debug("æ²¡æœ‰ä»æµäº‹ä»¶ä¸­è·å–åˆ°æœ€ç»ˆçŠ¶æ€ï¼Œå°è¯•ç›´æ¥è¿è¡Œ...")
                result = await self.training_graph.ainvoke(state, config)
                next_question = self._extract_next_question(result)
                
                if next_question:
                    logger.info(f"ğŸ¤– ä»ç›´æ¥è¿è¡Œç»“æœæå–é—®é¢˜: {next_question}")
                    async for msg in self._save_and_send_assistant_message(
                        digital_human_id, user_id, next_question
                    ):
                        yield msg
            
        except Exception as e:
            import traceback
            error_detail = traceback.format_exc()
            logger.error(f"è®­ç»ƒå¯¹è¯å¤„ç†å¤±è´¥: {str(e)}\nè¯¦ç»†é”™è¯¯:\n{error_detail}")
            yield json.dumps({
                "type": "error",
                "data": f"å¤„ç†å¤±è´¥: {str(e)}"
            }, ensure_ascii=False)
        finally:
            # ç¡®ä¿ commit æ€»æ˜¯è¢«è°ƒç”¨
            try:
                self.training_message_repo.commit()
            except Exception as commit_error:
                logger.error(f"æäº¤äº‹åŠ¡å¤±è´¥: {str(commit_error)}")
    
    
    
    
