from typing import Dict, List, Any, Generator, Optional
import json
from datetime import datetime
from sqlalchemy.orm import Session
from langchain_openai import ChatOpenAI
from langchain.schema import BaseMessage, HumanMessage, SystemMessage
from langgraph.graph import StateGraph, END
from pydantic import BaseModel

from app.services.knowledge_extractor import KnowledgeExtractor
from app.services.graph_service import GraphService
from app.core.models import DigitalHumanTrainingMessage
from app.core.logger import logger
from app.repositories.neomodel import GraphRepository
from app.core.config import settings


class TrainingState(BaseModel):
    messages: List[BaseMessage] = []
    digital_human_id: int
    user_id: int
    current_message: str = ""
    extracted_knowledge: Dict[str, Any] = {}
    knowledge_context: Dict[str, Any] = {}
    intent: str = ""
    next_question: str = ""
    should_extract: bool = False
    should_explore_deeper: bool = False
    conversation_stage: str = "initial"
    total_knowledge_points: int = 0
    categories: Dict[str, Any] = {}
    current_step: str = ""
    completed_steps: List[str] = []
    step_results: Dict[str, Any] = {}
    thinking_process: List[str] = []
    events: List[Dict[str, Any]] = []  # ‰∫ã‰ª∂ÈòüÂàóÔºåÁî®‰∫éÊµÅÂºèÈÄöÁü•


class DigitalHumanTrainingService:
    
    def __init__(
        self,
        db: Session,
        knowledge_extractor: KnowledgeExtractor,
        graph_service: GraphService
    ):
        self.db = db
        self.knowledge_extractor = knowledge_extractor
        self.graph_service = graph_service
        self.graph_repo = GraphRepository()
        
        self.llm = ChatOpenAI(
            api_key=settings.OPENAI_API_KEY,
            model="gpt-4o-mini",
            temperature=0.7
        )
        
        self.training_graph = self._build_training_graph()
    
    def _build_training_graph(self):
        workflow = StateGraph(TrainingState)
        
        workflow.add_node("intent_recognition", self._recognize_intent)
        workflow.add_node("knowledge_extraction", self._extract_knowledge)
        workflow.add_node("context_analysis", self._analyze_context)
        workflow.add_node("question_generation", self._generate_question)
        workflow.add_node("save_message", self._save_message)
        
        workflow.set_entry_point("intent_recognition")
        
        workflow.add_conditional_edges(
            "intent_recognition",
            self._route_by_intent,
            {
                "extract": "knowledge_extraction",
                "analyze": "context_analysis",
                "direct": "question_generation"
            }
        )
        
        workflow.add_edge("knowledge_extraction", "context_analysis")
        workflow.add_edge("context_analysis", "question_generation")
        workflow.add_edge("question_generation", "save_message")
        workflow.add_edge("save_message", END)
        
        return workflow.compile()
    
    def save_graph_visualization(self, output_dir: str = "graph_visualizations"):
        """‰øùÂ≠òÂ∑•‰ΩúÊµÅÂõæÁöÑÂèØËßÜÂåñ"""
        import os
        from datetime import datetime
        
        # ÂàõÂª∫ËæìÂá∫ÁõÆÂΩï
        os.makedirs(output_dir, exist_ok=True)
        
        # Ëé∑ÂèñÂõæÂØπË±°
        graph = self.training_graph.get_graph()
        
        # 1. Â∞ùËØïÁîüÊàê PNG ÂõæÁâá
        try:
            png_data = graph.draw_png()
            png_path = f"{output_dir}/training_graph.png"
            with open(png_path, "wb") as f:
                f.write(png_data)
            logger.info(f"‚úÖ ÂõæÂ∑≤‰øùÂ≠ò‰∏∫ PNG: {png_path}")
            return png_path
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Êó†Ê≥ïÁîüÊàê PNGÔºàÂèØËÉΩÈúÄË¶ÅÂÆâË£Ö graphvizÔºâ: {e}")
        
        # 2. Â¶ÇÊûú PNG Â§±Ë¥•ÔºåËá≥Â∞ë‰øùÂ≠ò Mermaid
        try:
            mermaid_text = graph.draw_mermaid()
            mermaid_path = f"{output_dir}/training_graph.mmd"
            with open(mermaid_path, "w") as f:
                f.write(mermaid_text)
            logger.info(f"‚úÖ ÂõæÂ∑≤‰øùÂ≠ò‰∏∫ Mermaid: {mermaid_path}")
            logger.info("üìä ÂèØÂú® https://mermaid.live Êü•Áúã")
            return mermaid_path
        except Exception as e:
            logger.error(f"‚ùå Êó†Ê≥ïÁîüÊàê‰ªª‰ΩïÂèØËßÜÂåñ: {e}")
            return None
    
    def get_graph_ascii(self) -> str:
        """Ëé∑Âèñ ASCII Ê†ºÂºèÁöÑÂõæ"""
        try:
            graph = self.training_graph.get_graph()
            return graph.print_ascii()
        except Exception as e:
            logger.error(f"Êó†Ê≥ïÁîüÊàê ASCII Âõæ: {e}")
            return "Êó†Ê≥ïÁîüÊàê ASCII Âõæ"
    
    def get_graph_mermaid(self) -> str:
        """Ëé∑Âèñ Mermaid Ê†ºÂºèÁöÑÂõæ"""
        try:
            graph = self.training_graph.get_graph()
            return graph.draw_mermaid()
        except Exception as e:
            logger.error(f"Êó†Ê≥ïÁîüÊàê Mermaid Âõæ: {e}")
            return "Êó†Ê≥ïÁîüÊàê Mermaid Âõæ"
    
    def _recognize_intent(self, state: TrainingState) -> TrainingState:
        # ËäÇÁÇπÂºÄÂßã‰∫ã‰ª∂
        state.events.append({
            "type": "node_start",
            "node": "intent_recognition",
            "message": "üîç ÂºÄÂßãÂàÜÊûêÁî®Êà∑ÊÑèÂõæ...",
            "timestamp": datetime.now().isoformat()
        })
        
        state.current_step = "recognizing_intent"
        state.thinking_process.append("Ê≠£Âú®ÂàÜÊûêÁî®Êà∑Ê∂àÊÅØÊÑèÂõæ...")
        
        # Ê∑ªÂä†ÊÄùËÄÉÊ≠•È™§
        state.events.append({
            "type": "thinking",
            "node": "intent_recognition",
            "message": "üí≠ Ëß£ÊûêÊ∂àÊÅØÂÜÖÂÆπÔºåËØÜÂà´ÂÖ≥ÈîÆ‰ø°ÊÅØ...",
            "timestamp": datetime.now().isoformat()
        })
        
        prompt = f"""
ÂàÜÊûê‰ª•‰∏ãÁî®Êà∑Ê∂àÊÅØÁöÑÊÑèÂõæÂíåÂÜÖÂÆπÁ±ªÂûãÔºö

Áî®Êà∑Ê∂àÊÅØ: {state.current_message}

ËØ∑Âà§Êñ≠Ôºö
1. ÊÑèÂõæÁ±ªÂûãÔºàinformation_sharing/question_asking/clarification/greeting/otherÔºâ
2. ÊòØÂê¶ÂåÖÂê´ÂèØÊèêÂèñÁöÑÁü•ËØÜÔºàyes/noÔºâ
3. ÂΩìÂâçÂØπËØùÈò∂ÊÆµÔºàinitial/exploring/deepening/concludingÔºâ

ËøîÂõûJSONÊ†ºÂºèÔºö
{{
    "intent": "...",
    "has_knowledge": true/false,
    "stage": "..."
}}
"""
        
        response = self.llm.invoke([SystemMessage(content=prompt)])
        try:
            result = json.loads(response.content)
        except json.JSONDecodeError as e:
            logger.error(f"ÊÑèÂõæËØÜÂà´ÂìçÂ∫îËß£ÊûêÂ§±Ë¥•: {e}")
            logger.error(f"ÂéüÂßãÂìçÂ∫î: {response.content}")
            raise ValueError(f"ÊÑèÂõæËØÜÂà´ÂìçÂ∫îÊ†ºÂºèÈîôËØØ: {str(e)}")
        
        state.intent = result.get("intent", "other")
        state.should_extract = result.get("has_knowledge", False)
        state.conversation_stage = result.get("stage", "exploring")
        
        state.step_results["intent_recognition"] = {
            "intent": state.intent,
            "should_extract": state.should_extract,
            "stage": state.conversation_stage
        }
        
        # ËäÇÁÇπÂÆåÊàê‰∫ã‰ª∂
        state.events.append({
            "type": "node_complete",
            "node": "intent_recognition",
            "message": f"‚úÖ ÊÑèÂõæËØÜÂà´ÂÆåÊàê: {state.intent}",
            "result": {
                "intent": state.intent,
                "stage": state.conversation_stage,
                "should_extract": state.should_extract
            },
            "timestamp": datetime.now().isoformat()
        })
        
        state.completed_steps.append("intent_recognition")
        state.thinking_process.append(f"ËØÜÂà´Âà∞ÊÑèÂõæ: {state.intent}, ÂØπËØùÈò∂ÊÆµ: {state.conversation_stage}")
        return state
    
    def _route_by_intent(self, state: TrainingState) -> str:
        if state.should_extract:
            return "extract"
        elif state.total_knowledge_points > 5:
            return "analyze"
        else:
            return "direct"
    
    async def _extract_knowledge(self, state: TrainingState) -> TrainingState:
        state.current_step = "extracting_knowledge"
        state.thinking_process.append("Ê≠£Âú®ÊèêÂèñÁü•ËØÜÁÇπ...")
        
        if not state.should_extract:
            state.extracted_knowledge = {"entities": [], "relationships": []}
            state.completed_steps.append("knowledge_extraction")
            return state
        
        extraction_result = await self.knowledge_extractor.extract(state.current_message)
        state.extracted_knowledge = extraction_result
        
        entity_count = len(extraction_result.get("entities", []))
        relationship_count = len(extraction_result.get("relationships", []))
        
        for entity in extraction_result.get("entities", []):
            await self._store_entity_to_graph(state.digital_human_id, entity)
        
        for relationship in extraction_result.get("relationships", []):
            await self._store_relationship_to_graph(state.digital_human_id, relationship)
        
        state.step_results["knowledge_extraction"] = {
            "entities_count": entity_count,
            "relationships_count": relationship_count,
            "extracted": extraction_result
        }
        
        state.completed_steps.append("knowledge_extraction")
        state.thinking_process.append(f"ÊèêÂèñÂà∞ {entity_count} ‰∏™ÂÆû‰Ωì, {relationship_count} ‰∏™ÂÖ≥Á≥ª")
        return state
    
    def _analyze_context(self, state: TrainingState) -> TrainingState:
        state.current_step = "analyzing_context"
        state.thinking_process.append("Ê≠£Âú®ÂàÜÊûêÁü•ËØÜÂõæË∞±‰∏ä‰∏ãÊñá...")
        
        context = self._get_current_context(state.digital_human_id)
        state.knowledge_context = context
        state.total_knowledge_points = context.get("total_knowledge_points", 0)
        state.categories = context.get("categories", {})
        
        if state.total_knowledge_points > 20 and not state.categories.get("hobby"):
            state.should_explore_deeper = True
        elif state.total_knowledge_points > 10 and len(state.categories) < 3:
            state.should_explore_deeper = True
        
        state.step_results["context_analysis"] = {
            "total_points": state.total_knowledge_points,
            "categories_count": len(state.categories),
            "should_explore_deeper": state.should_explore_deeper
        }
        
        state.completed_steps.append("context_analysis")
        state.thinking_process.append(f"Â∑≤‰∫ÜËß£ {state.total_knowledge_points} ‰∏™Áü•ËØÜÁÇπÔºåÊ∂µÁõñ {len(state.categories)} ‰∏™È¢ÜÂüü")
        return state
    
    def _generate_question(self, state: TrainingState) -> TrainingState:
        state.current_step = "generating_question"
        state.thinking_process.append("Ê≠£Âú®ÁîüÊàêÂºïÂØºÊÄßÈóÆÈ¢ò...")
        
        context_prompt = self._build_context_prompt(state)
        
        prompt = f"""
‰Ω†ÊòØ‰∏Ä‰∏™Ê≠£Âú®‰∫ÜËß£Áî®Êà∑ÁöÑÊï∞Â≠ó‰∫∫„ÄÇÂü∫‰∫éÂΩìÂâçÂØπËØùÁä∂ÊÄÅÔºåÁîüÊàê‰∏ã‰∏Ä‰∏™ÂºïÂØºÊÄßÈóÆÈ¢ò„ÄÇ

{context_prompt}

Ë¶ÅÊ±ÇÔºö
1. ÈóÆÈ¢òË¶ÅËá™ÁÑ∂„ÄÅÂèãÂ•Ω
2. Ê†πÊçÆÁî®Êà∑ÂàöÊâçÁöÑÂõûÁ≠îÂª∂‰º∏
3. ÈÄêÊ≠•Ê∑±ÂÖ•‰∫ÜËß£Áî®Êà∑
4. ‰∏çË¶ÅÈáçÂ§çÂ∑≤ÁªèÈóÆËøáÁöÑÂÜÖÂÆπ

ÁîüÊàê‰∏Ä‰∏™ÂºïÂØºÊÄßÈóÆÈ¢òÔºö
"""
        
        response = self.llm.invoke([SystemMessage(content=prompt)])
        state.next_question = response.content.strip()
        
        state.step_results["question_generation"] = {
            "question": state.next_question,
            "based_on_stage": state.conversation_stage
        }
        
        state.completed_steps.append("question_generation")
        state.thinking_process.append("Â∑≤ÁîüÊàêÂºïÂØºÊÄßÈóÆÈ¢ò")
        return state
    
    def _build_context_prompt(self, state: TrainingState) -> str:
        prompt_parts = []
        
        if state.current_message:
            prompt_parts.append(f"Áî®Êà∑ÂàöÊâçËØ¥: {state.current_message}")
        
        if state.extracted_knowledge.get("entities"):
            entities = state.extracted_knowledge["entities"]
            entity_names = [e.get("name") for e in entities[:3]]
            prompt_parts.append(f"ÊèêÂèñÂà∞ÁöÑÂÆû‰Ωì: {', '.join(entity_names)}")
        
        if state.total_knowledge_points > 0:
            prompt_parts.append(f"Â∑≤‰∫ÜËß£ÁöÑÁü•ËØÜÁÇπÊï∞: {state.total_knowledge_points}")
        
        if state.categories:
            cat_summary = []
            for cat, info in state.categories.items():
                if isinstance(info, dict) and info.get("count"):
                    cat_summary.append(f"{cat}({info['count']}‰∏™)")
            if cat_summary:
                prompt_parts.append(f"Â∑≤‰∫ÜËß£ÁöÑÈ¢ÜÂüü: {', '.join(cat_summary)}")
        
        prompt_parts.append(f"ÂΩìÂâçÂØπËØùÈò∂ÊÆµ: {state.conversation_stage}")
        
        return "\n".join(prompt_parts)
    
    async def _save_message(self, state: TrainingState) -> TrainingState:
        pass
    
    def _get_current_context(self, digital_human_id: int) -> Dict[str, Any]:
        try:
            query = """
            MATCH (dh:DigitalHuman {id: $dh_id})-[:HAS_KNOWLEDGE]->(k:Knowledge)
            WITH k, 
                 CASE 
                   WHEN k.type IN ['person', 'profession'] THEN 'profession'
                   WHEN k.type IN ['skill', 'technology'] THEN 'skill'
                   WHEN k.type IN ['project', 'product'] THEN 'project'
                   WHEN k.type IN ['organization', 'company'] THEN 'organization'
                   WHEN k.type IN ['hobby', 'interest'] THEN 'hobby'
                   ELSE 'other'
                 END as category
            RETURN category, collect(k.name) as items, count(k) as count
            """
            
            results = self.graph_repo.execute_query(query, {"dh_id": digital_human_id})
            
            context = {
                "total_knowledge_points": 0,
                "categories": {},
                "recent_entities": []
            }
            
            for row in results:
                category = row[0]
                items = row[1]
                count = row[2]
                context["categories"][category] = {
                    "count": count,
                    "items": items[:5]
                }
                context["total_knowledge_points"] += count
            
            return context
            
        except Exception as e:
            logger.error(f"Ëé∑ÂèñËÆ≠ÁªÉ‰∏ä‰∏ãÊñáÂ§±Ë¥•: {str(e)}")
            return {"total_knowledge_points": 0, "categories": {}}
    
    async def process_training_conversation(
        self,
        digital_human_id: int,
        user_message: str,
        user_id: int
    ) -> Generator[str, None, None]:
        user_msg = None
        state = None
        
        try:
            user_msg = DigitalHumanTrainingMessage(
                digital_human_id=digital_human_id,
                user_id=user_id,
                role="user",
                content=user_message
            )
            self.db.add(user_msg)
            self.db.flush()
            
            yield json.dumps({
                "type": "user_message",
                "id": user_msg.id,
                "data": user_message
            }, ensure_ascii=False)
            
            state = TrainingState(
                digital_human_id=digital_human_id,
                user_id=user_id,
                current_message=user_message,
                messages=[HumanMessage(content=user_message)]
            )
            
            yield json.dumps({
                "type": "thinking",
                "data": "ÂºÄÂßãÂàÜÊûêÂØπËØù..."
            }, ensure_ascii=False)
            
            # ËÆ∞ÂΩïÂ∑≤ÂèëÈÄÅÁöÑ‰∫ã‰ª∂Á¥¢ÂºïÔºåÈÅøÂÖçÈáçÂ§ç
            last_event_index = 0
            final_state = None  # ‰øùÂ≠òÊúÄÁªàÁä∂ÊÄÅ
            
            # ÂÆö‰πâ‰∏öÂä°Áõ∏ÂÖ≥ÁöÑ‰∏ªË¶ÅËäÇÁÇπ
            BUSINESS_NODES = {
                'intent_recognition', 'knowledge_extraction', 'context_analysis',
                'question_generation', 'save_message'
            }
            
            # ËÆ∞ÂΩïËäÇÁÇπÂºÄÂßãÊó∂Èó¥ÔºàÁî®‰∫éËÆ°ÁÆóÊâßË°åÊó∂Èó¥Ôºâ
            node_start_times = {}
            
            async for event in self.training_graph.astream_events(state, version="v2"):
                if event["event"] == "on_chain_start":
                    node_name = event.get("name", "")
                    
                    # Âè™ÂèëÈÄÅ‰∏öÂä°ËäÇÁÇπÁöÑ‰∫ã‰ª∂ÔºåËøáÊª§ÂÜÖÈÉ®ËäÇÁÇπ
                    if node_name in BUSINESS_NODES:
                        node_start_times[node_name] = datetime.now()
                        yield json.dumps({
                            "type": "node_start",
                            "node": node_name,
                            "data": f"‚è≥ ÂºÄÂßãÊâßË°å: {node_name}",
                            "timestamp": datetime.now().isoformat()
                        }, ensure_ascii=False)
                    elif node_name == "LangGraph":
                        # ‰øùÁïô‰∏ªÂõæÁöÑÂºÄÂßã‰∫ã‰ª∂
                        yield json.dumps({
                            "type": "workflow_start",
                            "data": "ÂºÄÂßãÊâßË°åÂ∑•‰ΩúÊµÅ",
                            "timestamp": datetime.now().isoformat()
                        }, ensure_ascii=False)
                
                elif event["event"] == "on_chain_stream":
                    # Â§ÑÁêÜÊµÅÂºèÊõ¥Êñ∞ÁöÑÁä∂ÊÄÅ
                    if "data" in event and isinstance(event["data"], dict):
                        chunk = event["data"].get("chunk", {})
                        
                        # ‰øùÂ≠òÂèØËÉΩÁöÑÊúÄÁªàÁä∂ÊÄÅ
                        if chunk:
                            logger.debug(f"üìä on_chain_stream chunk keys: {list(chunk.keys()) if isinstance(chunk, dict) else type(chunk)}")
                            # Â¶ÇÊûúchunkÂåÖÂê´next_questionÔºå‰øùÂ≠ò‰∏∫ÊúÄÁªàÁä∂ÊÄÅ
                            if isinstance(chunk, dict) and "next_question" in chunk:
                                final_state = chunk
                                logger.info(f"‚ú® Âú®ÊµÅ‰∫ã‰ª∂‰∏≠ÊâæÂà∞ÈóÆÈ¢ò: {chunk['next_question']}")
                
                elif event["event"] == "on_chain_end":
                    output = event.get("output", {})
                    node_name = event.get("name", "")
                    
                    # Âè™Â§ÑÁêÜ‰∏öÂä°ËäÇÁÇπ
                    if node_name in BUSINESS_NODES:
                        # ËÆ°ÁÆóÊâßË°åÊó∂Èó¥
                        execution_time = None
                        if node_name in node_start_times:
                            elapsed = (datetime.now() - node_start_times[node_name]).total_seconds()
                            execution_time = f"{elapsed:.2f}Áßí"
                        
                        # ÂáÜÂ§áËäÇÁÇπËæìÂá∫ÊëòË¶ÅÂíåËØ¶ÁªÜÊï∞ÊçÆ
                        output_summary = None
                        node_result = {}
                        
                        # Â∞ùËØï‰ªéËæìÂá∫‰∏≠ÊèêÂèñËäÇÁÇπÁâπÂÆöÁöÑÊï∞ÊçÆ
                        if isinstance(output, dict):
                            node_output = output.get(node_name, {})
                            
                            if node_name == "intent_recognition":
                                output_summary = "ËØÜÂà´Áî®Êà∑ÊÑèÂõæ"
                                # Â¶ÇÊûúËæìÂá∫ÂåÖÂê´ÊÑèÂõæ‰ø°ÊÅØÔºåÊèêÂèñÂÆÉ
                                if 'intent' in node_output:
                                    node_result['intent'] = node_output['intent']
                                    output_summary = f"ËØÜÂà´ÊÑèÂõæ: {node_output['intent']}"
                                    
                            elif node_name == "knowledge_extraction":
                                output_summary = "ÊèêÂèñÁü•ËØÜÁÇπ"
                                if 'extracted_knowledge' in node_output:
                                    entities = node_output['extracted_knowledge'].get('entities', [])
                                    node_result['entities_count'] = len(entities)
                                    output_summary = f"ÊèêÂèñ‰∫Ü {len(entities)} ‰∏™Áü•ËØÜÁÇπ"
                                    
                            elif node_name == "context_analysis":
                                output_summary = "ÂàÜÊûê‰∏ä‰∏ãÊñá"
                                if 'total_knowledge_points' in node_output:
                                    node_result['total_points'] = node_output['total_knowledge_points']
                                    output_summary = f"ÂΩìÂâçÂÖ± {node_output['total_knowledge_points']} ‰∏™Áü•ËØÜÁÇπ"
                                    
                            elif node_name == "question_generation":
                                output_summary = "ÁîüÊàêÂºïÂØºÈóÆÈ¢ò"
                                if 'next_question' in node_output:
                                    # Êà™ÂèñÈóÆÈ¢òÁöÑÂâç50‰∏™Â≠óÁ¨¶‰Ωú‰∏∫ÊëòË¶Å
                                    question_preview = node_output['next_question'][:50]
                                    node_result['question_preview'] = question_preview
                                    output_summary = f"ÁîüÊàêÈóÆÈ¢ò: {question_preview}..."
                                    
                            elif node_name == "save_message":
                                output_summary = "‰øùÂ≠òÊ∂àÊÅØËÆ∞ÂΩï"
                                node_result['saved'] = True
                        
                        yield json.dumps({
                            "type": "node_complete",
                            "node": node_name,
                            "data": f"‚úÖ ÂÆåÊàêÊâßË°å: {node_name}",
                            "execution_time": execution_time,
                            "summary": output_summary,
                            "result": node_result if node_result else None,
                            "timestamp": datetime.now().isoformat()
                        }, ensure_ascii=False)
                    
                    elif node_name == "LangGraph":
                        # Â∑•‰ΩúÊµÅÂÆåÊàê‰∫ã‰ª∂
                        logger.info(f"üéØ Â∑•‰ΩúÊµÅÂÆåÊàê!")
                        yield json.dumps({
                            "type": "workflow_complete",
                            "data": "Â∑•‰ΩúÊµÅÊâßË°åÂÆåÊàê",
                            "timestamp": datetime.now().isoformat()
                        }, ensure_ascii=False)
                    
                    # Â¶ÇÊûúÊòØ question_generation ËäÇÁÇπÂÆåÊàêÔºåËæìÂá∫ÁîüÊàêÁöÑÈóÆÈ¢ò
                    if node_name == "question_generation" and output:
                        # Ë∞ÉËØïÔºöÊü•Áúã output ÁöÑÂÜÖÂÆπ
                        logger.debug(f"question_generation output type: {type(output)}")
                        
                        # ÊèêÂèñÊúÄÁªàÁöÑÈóÆÈ¢ò - output ÂèØËÉΩÊòØ TrainingState Êàñ dict
                        final_question = None
                        
                        # Â¶ÇÊûú output Êúâ keys ÊñπÊ≥ïÔºåËØ¥ÊòéÊòØ dict
                        if hasattr(output, 'keys'):
                            # Â∞ùËØï‰ªé dict ‰∏≠Ëé∑Âèñ
                            if 'next_question' in output:
                                final_question = output['next_question']
                            # ÊàñËÄÖÂ∞ùËØï‰ªéÂµåÂ•óÁöÑ state ‰∏≠Ëé∑Âèñ
                            elif isinstance(output, dict) and 'state' in output:
                                state_obj = output['state']
                                if hasattr(state_obj, 'next_question'):
                                    final_question = state_obj.next_question
                                elif isinstance(state_obj, dict) and 'next_question' in state_obj:
                                    final_question = state_obj['next_question']
                        # Â¶ÇÊûú output ÊòØ TrainingState ÂØπË±°
                        elif hasattr(output, 'next_question'):
                            final_question = output.next_question
                        
                        logger.info(f"‚ú® ÊèêÂèñÂà∞ÁöÑÈóÆÈ¢ò: {final_question}")
                        
                        if final_question:
                            # ‰øùÂ≠òÂä©ÊâãÊ∂àÊÅØ
                            assistant_msg = DigitalHumanTrainingMessage(
                                digital_human_id=digital_human_id,
                                user_id=user_id,
                                role="assistant",
                                content=final_question
                            )
                            self.db.add(assistant_msg)
                            self.db.flush()
                            
                            # ÂèëÈÄÅÂä©ÊâãÈóÆÈ¢ò‰∫ã‰ª∂
                            yield json.dumps({
                                "type": "assistant_question",
                                "id": assistant_msg.id,
                                "data": final_question
                            }, ensure_ascii=False)
                    
                    # Ê£ÄÊü• output ÊòØÂê¶ÊúâÊ≠£Á°ÆÁöÑÂ±ûÊÄß
                    if hasattr(output, 'completed_steps'):
                        # output ÊòØ TrainingState ÂØπË±°
                        completed_steps = output.completed_steps
                        intent = output.intent
                        stage = output.conversation_stage
                        should_extract = output.should_extract
                        extracted_knowledge = output.extracted_knowledge
                        total_points = output.total_knowledge_points
                        categories = output.categories
                        should_explore = output.should_explore_deeper
                        next_question = output.next_question
                        thinking_process = output.thinking_process
                        events = output.events
                    else:
                        # output ÊòØÂ≠óÂÖ∏ÔºåÈúÄË¶ÅÊèêÂèñÂÄº
                        completed_steps = output.get('completed_steps', [])
                        intent = output.get('intent', '')
                        stage = output.get('conversation_stage', '')
                        should_extract = output.get('should_extract', False)
                        extracted_knowledge = output.get('extracted_knowledge', {})
                        total_points = output.get('total_knowledge_points', 0)
                        categories = output.get('categories', {})
                        should_explore = output.get('should_explore_deeper', False)
                        next_question = output.get('next_question', '')
                        thinking_process = output.get('thinking_process', [])
                        events = output.get('events', [])
                    
                    # ÂèëÈÄÅÊú™Â§ÑÁêÜÁöÑ‰∫ã‰ª∂
                    if events and isinstance(events, list) and len(events) > last_event_index:
                        new_events = events[last_event_index:]
                        for evt in new_events:
                            yield json.dumps(evt, ensure_ascii=False)
                        last_event_index = len(events)
                    
                    if "intent_recognition" in completed_steps:
                        yield json.dumps({
                            "type": "intent_recognized",
                            "data": {
                                "intent": intent,
                                "stage": stage,
                                "should_extract": should_extract
                            }
                        }, ensure_ascii=False)
                    
                    if "knowledge_extraction" in completed_steps and extracted_knowledge.get("entities"):
                        user_msg.extracted_knowledge = extracted_knowledge
                        user_msg.extraction_metadata = {
                            "extraction_time": datetime.now().isoformat(),
                            "intent": intent,
                            "stage": stage,
                            "thinking_process": thinking_process
                        }
                        
                        yield json.dumps({
                            "type": "knowledge_extracted",
                            "id": user_msg.id,
                            "data": extracted_knowledge["entities"]
                        }, ensure_ascii=False)
                    
                    if "context_analysis" in completed_steps:
                        yield json.dumps({
                            "type": "context_analyzed",
                            "data": {
                                "total_points": total_points,
                                "categories": list(categories.keys()) if categories else [],
                                "should_explore_deeper": should_explore
                            }
                        }, ensure_ascii=False)
                    
                    if "question_generation" in completed_steps and next_question:
                        assistant_msg = DigitalHumanTrainingMessage(
                            digital_human_id=digital_human_id,
                            user_id=user_id,
                            role="assistant",
                            content=next_question
                        )
                        self.db.add(assistant_msg)
                        self.db.flush()
                        
                        yield json.dumps({
                            "type": "assistant_question",
                            "id": assistant_msg.id,
                            "data": next_question
                        }, ensure_ascii=False)
                
                elif event["event"] == "on_chain_stream":
                    if event.get("data", {}).get("thinking_process"):
                        for thought in event["data"]["thinking_process"]:
                            yield json.dumps({
                                "type": "thinking",
                                "data": thought
                            }, ensure_ascii=False)
            
            # Âú®ÊµÅÁªìÊùüÂêéÔºåÊ£ÄÊü•ÊòØÂê¶ÊúâÊúÄÁªàÁä∂ÊÄÅ
            if final_state and "next_question" in final_state:
                logger.info(f"ü§ñ ‰ªéÊúÄÁªàÁä∂ÊÄÅÊèêÂèñÈóÆÈ¢ò: {final_state['next_question']}")
                
                # ‰øùÂ≠òÂä©ÊâãÊ∂àÊÅØ
                assistant_msg = DigitalHumanTrainingMessage(
                    digital_human_id=digital_human_id,
                    user_id=user_id,
                    role="assistant",
                    content=final_state['next_question']
                )
                self.db.add(assistant_msg)
                self.db.flush()
                
                # ÂèëÈÄÅÂä©ÊâãÈóÆÈ¢ò‰∫ã‰ª∂
                yield json.dumps({
                    "type": "assistant_question",
                    "id": assistant_msg.id,
                    "data": final_state['next_question']
                }, ensure_ascii=False)
            else:
                # Â¶ÇÊûúÊ≤°Êúâ‰ªéÊµÅ‰∏≠Ëé∑ÂèñÂà∞Áä∂ÊÄÅÔºåÂ∞ùËØïÁõ¥Êé•ËøêË°å‰∏ÄÊ¨°Ëé∑ÂèñÁªìÊûú
                logger.debug("Ê≤°Êúâ‰ªéÊµÅ‰∫ã‰ª∂‰∏≠Ëé∑ÂèñÂà∞ÊúÄÁªàÁä∂ÊÄÅÔºåÂ∞ùËØïÁõ¥Êé•ËøêË°å...")
                try:
                    result = await self.training_graph.ainvoke(state)
                    logger.info(f"üì¶ Áõ¥Êé•ËøêË°åÁªìÊûúÁ±ªÂûã: {type(result)}")
                    
                    # Â∞ùËØïÂ§öÁßçÊñπÂºèÊèêÂèñ next_question
                    next_question = None
                    if result:
                        # ÂÖàÂ∞ùËØï‰Ωú‰∏∫Â≠óÂÖ∏ËÆøÈóÆÔºàAddableValuesDict ÊòØÂ≠óÂÖ∏Á±ªÂûãÔºâ
                        if hasattr(result, '__getitem__') and 'next_question' in result:
                            next_question = result['next_question']
                        # ÂÜçÂ∞ùËØï‰Ωú‰∏∫ÂØπË±°Â±ûÊÄßËÆøÈóÆ
                        elif hasattr(result, 'next_question'):
                            next_question = result.next_question
                        
                        # ËÆ∞ÂΩïÁªìÊûúËØ¶ÊÉÖ
                        if hasattr(result, '__dict__'):
                            logger.info(f"üì¶ ÁªìÊûúÂ±ûÊÄß: {list(vars(result).keys())[:10]}")
                    
                    if next_question:
                        logger.info(f"ü§ñ ‰ªéÁõ¥Êé•ËøêË°åÁªìÊûúÊèêÂèñÈóÆÈ¢ò: {next_question}")
                        
                        # ‰øùÂ≠òÂä©ÊâãÊ∂àÊÅØ
                        assistant_msg = DigitalHumanTrainingMessage(
                            digital_human_id=digital_human_id,
                            user_id=user_id,
                            role="assistant",
                            content=result.next_question
                        )
                        self.db.add(assistant_msg)
                        self.db.flush()
                        
                        # ÂèëÈÄÅÂä©ÊâãÈóÆÈ¢ò‰∫ã‰ª∂
                        yield json.dumps({
                            "type": "assistant_question",
                            "id": assistant_msg.id,
                            "data": next_question
                        }, ensure_ascii=False)
                except Exception as e:
                    # ÂøΩÁï•Â±ûÊÄßÈîôËØØÔºåÂõ†‰∏∫Êàë‰ª¨Â∑≤ÁªèÊàêÂäüÊèêÂèñ‰∫ÜÈóÆÈ¢ò
                    if "has no attribute 'next_question'" not in str(e):
                        logger.debug(f"Áõ¥Êé•ËøêË°åÂá∫Áé∞ÂºÇÂ∏∏: {e}")
            
            self.db.commit()
            
        except AttributeError as e:
            if "'async_generator' object has no attribute 'astream_events'" in str(e):
                yield json.dumps({
                    "type": "info",
                    "data": "‰ΩøÁî®Â§áÁî®ÊµÅÂºèÂ§ÑÁêÜÊñπÊ≥ï..."
                }, ensure_ascii=False)
                
                result = await self.training_graph.ainvoke(state)
                
                for thought in result.thinking_process:
                    yield json.dumps({
                        "type": "thinking",
                        "data": thought
                    }, ensure_ascii=False)
                
                if result.step_results.get("intent_recognition"):
                    yield json.dumps({
                        "type": "intent_recognized",
                        "data": result.step_results["intent_recognition"]
                    }, ensure_ascii=False)
                
                if result.extracted_knowledge.get("entities"):
                    user_msg.extracted_knowledge = result.extracted_knowledge
                    user_msg.extraction_metadata = {
                        "extraction_time": datetime.now().isoformat(),
                        "intent": result.intent,
                        "stage": result.conversation_stage
                    }
                    
                    yield json.dumps({
                        "type": "knowledge_extracted",
                        "id": user_msg.id,
                        "data": result.extracted_knowledge["entities"]
                    }, ensure_ascii=False)
                
                if result.step_results.get("context_analysis"):
                    yield json.dumps({
                        "type": "context_analyzed",
                        "data": result.step_results["context_analysis"]
                    }, ensure_ascii=False)
                
                assistant_msg = DigitalHumanTrainingMessage(
                    digital_human_id=digital_human_id,
                    user_id=user_id,
                    role="assistant",
                    content=result.next_question
                )
                self.db.add(assistant_msg)
                self.db.flush()
                self.db.commit()
                
                yield json.dumps({
                    "type": "assistant_question",
                    "id": assistant_msg.id,
                    "data": result.next_question
                }, ensure_ascii=False)
            else:
                raise e
        except Exception as e:
            logger.error(f"ËÆ≠ÁªÉÂØπËØùÂ§ÑÁêÜÂ§±Ë¥•: {str(e)}")
            yield json.dumps({
                "type": "error",
                "data": f"Â§ÑÁêÜÂ§±Ë¥•: {str(e)}"
            }, ensure_ascii=False)
    
    
    async def _store_entity_to_graph(
        self,
        digital_human_id: int,
        entity: Dict[str, Any]
    ):
        try:
            query = """
            MERGE (dh:DigitalHuman {id: $dh_id})
            MERGE (k:Knowledge {
                name: $name,
                digital_human_id: $dh_id
            })
            SET k.type = $type,
                k.types = $types,
                k.confidence = $confidence,
                k.properties = $properties,
                k.updated_at = datetime()
            MERGE (dh)-[r:HAS_KNOWLEDGE]->(k)
            SET r.updated_at = datetime()
            """
            
            self.graph_repo.execute_query(query, {
                "dh_id": digital_human_id,
                "name": entity.get("name"),
                "type": entity.get("type", "unknown"),
                "types": json.dumps(entity.get("types", [])),
                "confidence": entity.get("confidence", 0.5),
                "properties": json.dumps(entity.get("properties", {}))
            })
            
        except Exception as e:
            logger.error(f"Â≠òÂÇ®ÂÆû‰ΩìÂà∞ÂõæÊï∞ÊçÆÂ∫ìÂ§±Ë¥•: {str(e)}")
    
    async def _store_relationship_to_graph(
        self,
        digital_human_id: int,
        relationship: Dict[str, Any]
    ):
        try:
            query = """
            MATCH (k1:Knowledge {
                name: $source,
                digital_human_id: $dh_id
            })
            MATCH (k2:Knowledge {
                name: $target,
                digital_human_id: $dh_id
            })
            MERGE (k1)-[r:RELATES_TO]->(k2)
            SET r.relation_type = $relation_type,
                r.confidence = $confidence,
                r.properties = $properties,
                r.updated_at = datetime()
            """
            
            self.graph_repo.execute_query(query, {
                "dh_id": digital_human_id,
                "source": relationship.get("source"),
                "target": relationship.get("target"),
                "relation_type": relationship.get("relation_type"),
                "confidence": relationship.get("confidence", 0.5),
                "properties": json.dumps(relationship.get("properties", {}))
            })
            
        except Exception as e:
            logger.error(f"Â≠òÂÇ®ÂÖ≥Á≥ªÂà∞ÂõæÊï∞ÊçÆÂ∫ìÂ§±Ë¥•: {str(e)}")
    
    
    
